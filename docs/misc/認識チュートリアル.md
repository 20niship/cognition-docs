
Try pandoc!

pandoc --from mediawiki --to markdown --no-highlight

 
from
Input


Add support file
Template
to
 Standalone
 Embed resources
 Citeproc
 TOC
 Number sections
permalink
download example as JSON

![](1508683496340-cognitive-hazard.jpg "1508683496340-cognitive-hazard.jpg"){width="400"}

`愚かな人間なので「`[`長いページ`](特別:長いページ "wikilink")`で上位ランキング入りする」という副目標がないと頑張れないわけです．なので，完成するまでページ分割はされません．`

`院試に合格したら続きを書きます．--`[`KYabuuchi`](利用者:Kyabuuchi "wikilink")` (`[`トーク`](利用者・トーク:Kyabuuchi "wikilink")`) 2019年8月21日 (水) 18:41 (JST)`\
`落ちたら書きません．`\
`ちなみに`[`こちら`](https://www.youtube.com/watch?v=_xduquXZuxo)`がオススメです．`

## 概要

-   このチュートリアルの最終目標ですが，[これ](https://www.youtube.com/watch?v=qRrMHaO6NpE)ができるようになることにしようと思いました．
    -   単眼でのVisualOdometryではスケール不定性が残るので，ステレオカメラであるZEDで撮影した動画を使ってOdometryをしてもらいます．
-   ところで，[籔内健人/認識](籔内健人/認識 "wikilink")も見るといいかもしれません．
    -   あちらは雑記に近い
-   まとめとして[このスライド](https://docs.google.com/presentation/d/1WGq4eIUgLeIXhyGiUKm_zmSK3LDjqNLDZc9SAlyIbu8/edit?usp=sharing)を見ると良いかもしれません．
    -   数式の整理などに役立つような気がします．ただし参考になるのはp23までで，それ以降は僕のメモです．
-   ところで，[特別:長いページ](特別:長いページ "wikilink")で[ドキュメント/制御/libcraftチュートリアル](ドキュメント/制御/libcraftチュートリアル "wikilink")を追い抜きました．\--[KYabuuchi](利用者:Kyabuuchi "wikilink")
    ([トーク](利用者・トーク:Kyabuuchi "wikilink")) 2019年1月6日 (日)
    15:46 (JST)
-   依頼されたときから何を書けばいいのかわからなかったのだが，自分が好きなことを書こうと思った．
-   昨今はCVといえばDNNみたいな風潮だが，古典CVも楽しいのである．
-   カメラ幾何とか絡めてOpenCVを学びつつ，素敵なものを作る感じの課題にしたいと思う12/30であった．
-   SfMがロボコンで必要になることはない気がしますが，CVの総合力が付くと思います．
-   [よき資料1](http://www.wakayama-u.ac.jp/~wuhy/CV07.pdf)
-   [よき資料2](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/03_odometry.pdf)

```{=html}
<!-- -->
```
-   どうやら見ている人が何人かいるという話を聞いたので再開しました．
-   とりあえず[こちら](https://www.youtube.com/watch?v=2kZVEUGLgy4&list=PLY4pDevDNKpNDH3Pp9fwYJ3YiKIUdb6jL&index=14&t=0s)をごらんください．

```{=html}
<!-- -->
```
-   OpenCVに関するいわゆる同人誌を買ってみました．
    -   [はじめてじゃないOpenCV](https://webdav.robotech.t.u-tokyo.ac.jp/webdav/private/kyabuuchi/CV/opencv-book-vol1.pdf)(￥500)
    -   [OpenCV4.0
        ガイドブック](https://webdav.robotech.t.u-tokyo.ac.jp/webdav/private/kyabuuchi/CV/opencv-book-vol2.pdf)(￥500)
        -   どちらもおくまった話なので，少なくともこのチュートリアルのOpenCVの章はできるぐらいでないと見ても意味がないと思います．

```{=html}
<!-- -->
```
-   練習問題とかでこのページにある画像をダウンロードするときは，リンク先に移動してやらないと正しい解像度のそれが得られないです．

```{=html}
<!-- -->
```
-   [線形代数で3次元復元](https://www.amazon.co.jp/%E7%B7%9A%E5%BD%A2%E4%BB%A3%E6%95%B0%E3%81%A73%E6%AC%A1%E5%85%83%E5%BE%A9%E5%85%83-%E7%8E%89%E6%9C%A8%E5%BE%B9-ebook/dp/B00JWRAVXS/ref=sr_1_1?__mk_ja_JP=%E3%82%AB%E3%82%BF%E3%82%AB%E3%83%8A&keywords=%E7%B7%9A%E5%BD%A2%E4%BB%A3%E6%95%B0%E3%81%A7%EF%BC%93%E6%AC%A1%E5%85%83%E5%BE%A9%E5%85%83&qid=1561998019&s=gateway&sr=8-1)という本がamazonで無料で売ってました．
    -   大した内容はありませんでした．

## OpenCV

![](1546194675388-opencv.png "1546194675388-opencv.png"){width="400"}

-   OpenCVの素敵な歴史
    -   OpenCVはオープンソースのコンピュータビジョンライブラリで，CとC++で書かれていて，Python,Ruby,Matlab，Javaとかでも動くライブラリです．
    -   なんというか，コンピュータビジョンを扱うときの常套手段的な存在です．
    -   コンピュータビジョンのアルゴリズムが多数実装されているだけでなく，動画像，静止画の入出力，GUI機能も提供しています．
-   OpenCVはIntelが1999年にプロジェクトが開始しました．
    -   github上での最新のバージョンは4.0.1ですね．(2018/12/31)
    -   結構活発に新しいバージョンがリリースされます．
-   OpenCVは2.x系とそれ以降のバージョンで結構APIが変わっていて，互換性がありません．
    -   悲しいことに日本語のOpenCVの幾つかの記事は2.x系のほうがまだ多いような気がします．
    -   見極める目安として，2.x系ではAPIの多くはcvHogeHogeみたいになってますが，3.x以降であればcv名前空間にAPIが覆われており，cv::hogeHogeみたいになってます．
        -   いうて，それだけ直してしまえば動くものも多いです．
        -   さらに悲しいことに，2.x系でincludeファイルのディレクトリ構造がinclude/opencv2/core/core.hppみたいになっていたのを3.x系でも引きずっており
        -   OpenCV3.4を使っているのに#include
            \<opencv2/core/core.hpp\>を強いられます．
        -   OpenCV4からは何を血迷ったのかinclude/opencv4/opencv2/core/core.hppみたいになります．
-   BSDライセンスで配布されているらしく，学術的な用途だけでなく(一部を除いて)商用目的も利用できます．

```{=html}
<!-- -->
```
-   OpenCVに関する情報は[このへん](https://docs.opencv.org/3.4.3/index.html)を見るとはかどります．
    -   あんまり困ることは無いですが，自分が利用しているバージョンのページを見るようにしましょう．
    -   先のページの中にもTutorialみたいなのがあります．

```{=html}
<!-- -->
```
-   以下ではOpenCVのインストールの仕方から，基本的な機能の使い方をサンプルプログラムを交えながら紹介しようと思います．

### Install

-   aptで入れる方法と，自前でsourceをbuildする方法があります．
    -   aptですれば色々手っ取り早いですが，自前でbuildしたほうがオプションを付けられたりするので素敵です．
    -   軽くいじるだけであれば，aptで入れればいいような気もします．
    -   build時にむっちゃファイルを生成してくるのと，むっちゃ時間がかかるのがネックです．
        -   機能マシマシにすると10Gぐらい持ってかれる気がします．
-   ところで，opencvには基本的な機能(というか本体)の[opencv](https://github.com/opencv/opencv/)と発展的な機能(ライセンスが険しいのもこちら)の[opencv_contirb](https://github.com/opencv/opencv_contrib)があります．
    -   あと，テスト用のファイルだけが格納された[opencv_extra](https://github.com/opencv/opencv_extra)なるものもあります．これは気にしなくていいです．
    -   このチュートリアル?をやるだけならcontribは不要だと思います．
        -   あるに越したことはないです．なくても動くコードをサンプルとしての載せるように心がけます．
    -   contirbには怪しくて発展的なmoduleがあったりするので遊んでみるのもいいかもしれません．

`ところで，Ubuntu18.04を想定しています．`

-   一応，公式のRequired Packagesを確認しましょう．
-   GCC 4.4.x or later
-   CMake 2.8.7 or higher
-   Git
-   GTK+2.x or higher, including headers (libgtk2.0-dev)
-   pkg-config
-   Python 2.6 or later and Numpy 1.5 or later with developer packages
    (python-dev, python-numpy)
-   ffmpeg or libav development packages: libavcodec-dev,
    libavformat-dev, libswscale-dev
-   \[optional\] libtbb2 libtbb-dev
-   \[optional\] libdc1394 2.x
-   \[optional\] libjpeg-dev, libpng-dev, libtiff-dev, libjasper-dev,
    libdc1394-22-dev
-   \[optional\] CUDA Toolkit 6.5 or higher
-   そうですね．

```{=html}
<!-- -->
```
-   最初にも?述べましたようにPythonからでもOpenCVを利用することができます．
-   Python向けにインストールする方法はもまた２つあります．
    -   一つはPre-builtなバイナリを利用する方法
    -   もう一つはsouceを自前でbuildする方法です．
        -   後者はC++向けのライブラリを自前でbuildするときと同様です．
-   どちらを選ぶかは先の議論を参考にしてください．
    -   C++向けのそれをaptで入れたならPython向けもaptで入れれば良いし，C++向けをbuildするならついでにPython向けもbuildすればいいと思います．

```{=html}
<!-- -->
```
-   Pythonで利用する場合はnumpyが必須です．
-   IPythonとmatplotlibがOptionalです．

#### 方法1: aptで入れる場合 {#方法1_aptで入れる場合}

`$ sudo apt install libopencv-dev`

-   終わりです．
-   やったことないから知らないのですけど，aptからはopencv_contribは入れられない雰囲気ですね．
-   ちゃんと入れば，以下のようなコマンドが叩けるようになるような気がします．

`$ opencv_version #バージョンが出力される．Ubuntu18.04なら標準で3.2.0らしい`

-   Pythonの場合は以下です．

`$ sudo apt install python-opencv`

-   終わりです．

`$ pip3 install opencv-python # こちらでも可能です`

#### 方法2: 自前でbuildする場合 {#方法2_自前でbuildする場合}

`$ git clone --depth 1 `[`https://github.com/opencv/opencv.git`](https://github.com/opencv/opencv.git)\
`$ #必要ならば git clone --depth 1 `[`https://github.com/opencv/opencv_contrib.git`](https://github.com/opencv/opencv_contrib.git)` `\
`$ cd opencv`\
`$ mkdir build`\
`$ cd build`

-   ここでオプションを色々指示できるので，幾つか紹介します．

  --------------------------- ----------------------------------------------------------
  オプション                  効果
  並列化系                    
  WITH_CUDA                   CUDAを有効にする
  WITH_OPENCL                 CLを有効にする
  WITH_TBB                    intel TBBを有効にする
  ビルド対象                  
  BUILD_DOCS                  ドキュメントのビルド
  BUILD_EXAMPLES              サンプルプログラムのビルド
  BUILD_TESTS                 テストプログラムをビルド
  その他                      
  WITH_FFMPEG                 動画入出力機能でFFMPEGを使用する
  WITH_QT                     Qtを使ってGUI機能を拡張する
  OPENCV_EXTRA_MODULES_PATH   opencv_contribのディレクトリを渡します(不要なら空白でOK)
  OPENCV_ENABLE_NONFREE       ライセンスフリーではないものを使えるようにする
                              
  --------------------------- ----------------------------------------------------------

-   例えば以下のような感じで叩きます．

`$ cmake \`\
`-D CMAKE_BUILD_TYPE=RELEASE\`\
`-D CMAKE_INSTALL_PREFIX=/usr/local\`\
`-D WITH_FFMPEG=ON\`\
`-D WITH_TBB=ON\`\
`-D WITH_GTK=ON\`\
`-D WITH_V4L=ON\`\
`-D WITH_OPENGL=ON\`\
`-D WITH_QT=ON\`\
`-D WITH_VTK=ON\`\
`-D OPENCV_EXTRA_MODULES_PATH=path/to/opencv_contrib/modules\`\
`-D OPENCV_ENABLE_NONFREE=ON\`\
`-D BUILD_opencv_python3=ON\`\
`-D BUILD_opencv_python2=OFF\`\
`-D BUILD_EXAMPLES=ON\`\
` ..`

-   よくわからないなら以下でも問題ないです．

`$ cmake -D WITH_QT=ON .. `

-   Python向けのライブラリはPythonとnumpyがインストールされているなら勝手に(?)ビルドオプションが有効化されます．
-   cmakeしたときに以下のような文字列が出ていればbuildされます．(なお以下の出力はPython2向けとPython3向けの両方がbuildされるときのそれです)

`--   Python 2:`\
`--     Interpreter:                 /usr/bin/python2.7 (ver 2.7.6)`\
`--     Libraries:                   /usr/lib/x86_64-linux-gnu/libpython2.7.so (ver 2.7.6)`\
`--     numpy:                       /usr/lib/python2.7/dist-packages/numpy/core/include (ver 1.8.2)`\
`--     packages path:               lib/python2.7/dist-packages`\
`--`\
`--   Python 3:`\
`--     Interpreter:                 /usr/bin/python3.4 (ver 3.4.3)`\
`--     Libraries:                   /usr/lib/x86_64-linux-gnu/libpython3.4m.so (ver 3.4.3)`\
`--     numpy:                       /usr/lib/python3/dist-packages/numpy/core/include (ver 1.8.2)`\
`--     packages path:               lib/python3.4/dist-packages`

-   続きです．

`$ make -j4 #PCの性能にも依るが数時間生贄に捧げる覚悟で`\
`$ sudo make install`

-   終わりです．
-   ちゃんと入れば，以下のようなコマンドが叩けるようになるような気がします．

`$ opencv_version # opencvのバージョンが出力される`

#### 練習

-   以下のファイルを用意して動かせ．

```{=html}
<div align="center">
```
CMakeLists.txt

```{=html}
</div>
```
``` {.cmake .numberLines}
 cmake_minimum_required(VERSION 3.0)
 project(main)
 find_package(OpenCV REQUIRED)
 add_executable(main main.cpp)
 target_link_libraries(main ${OpenCV_LIBS})
```

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    cv::Mat image = cv::Mat::zeros(480, 640, CV_8UC3);
    cv::putText(image, "Hello World", cv::Point2i(150, 240), cv::FONT_HERSHEY_PLAIN, 5, cv::Scalar(255, 0, 0), 2);
    cv::namedWindow("window", cv::WINDOW_NORMAL);
    cv::imshow("window", image);
    cv::waitKey(0);
    cv::destroyAllWindows();
}
```

-   うまく行けば以下のような画像が表示される．
    -   そして，何かしらのキーを押すと閉じるはずである．

![](1546188874327-Screenshot_from_2018-12-31_01-53-43.png "1546188874327-Screenshot_from_2018-12-31_01-53-43.png"){width="400"}

-   Python版は以下である．

```{=html}
<div align="center">
```
main.py

```{=html}
</div>
```
``` {.python .numberLines}
import numpy as np
import cv2 as cv
img = np.zeros((480, 640, 3), np.uint8)
cv.putText(img, "Hello World", (150, 240), cv.FONT_HERSHEY_PLAIN, 5, (255, 0, 0), 2)
cv.namedWindow("window", cv.WINDOW_NORMAL)
cv.imshow('window', img)
cv.waitKey(0)
cv.destroyAllWindows()
```

`あの人(?)，ライブ開催していたんですね．`

![](1546194554838-helloworld.jpg "1546194554838-helloworld.jpg"){width="400"}

### OpenCVの注意すべき基本的なこと

`色はBlue Green Redの順番であるということ．`

-   先の例でcv::Scalar(255,0,0)としているが，これは青:255(最大)，緑:0(最小)，赤:0(最小)を意味しています．
    -   そのため，黒い背景(0,0,0)で埋められた行列の上に青い文字でHello
        Worldと書かれました．
        -   そういうことです．
-   ところで，CV_RGB(r,g,b)みたいなマクロはあるのに，CV_BGR(b,g,r)はありません．
    -   次の意味です． #define CV_BGR(r,g,b) cv::Scalar((b),(g),(r),0)
-   色の順番は良く勘違いするので注意してください．
    -   間違える例として，グレースケール化するときに
    -   cv::cvtColor(src_image,dst_image,cv::COLOR_BGR2GRAY)を用いるのですが，
    -   cv::cvtColor(src_image,dst_image,cv::COLOR_RGB2GRAY)ということもできます．
    -   [籔内健人/認識#OpenCVのBGR vs
        RGB](籔内健人/認識#OpenCVのBGR_vs_RGB "wikilink")にも述べられています．

`cv::Mat(画像を扱うときに用いる基本的な型)は縦x横で指定するが，cv::Point(空間や画像上の点を扱うときに用いる基本的な型)などは(x,y)つまり(横,縦)で指定する．`

-   先の例でimageを生成するときに，(480,640)で画像サイズを指定していますが，出てきた画像は横長ですよね．
    -   そういうことです．
-   しかし，文字を描画するcv::putText()の位置を指定する引数の型はcv::Pointなので(x,y)で指定します．
    -   よく見るとそうですよね．
        -   そういうことです．
-   これはcv::Matが普通の数学の行列みたいな機能だからそうなっています．
    -   先の例では行列の(i,j)要素に0\~255(unsigned
        char)の正整数が3個入ったような行列として振る舞っているのです．
    -   もちろん，回転行列みたいなnxnの浮動小数点の数値が1個づつ入るようなMatも作ることができます．
    -   次の章で扱うような気がします．
-   勘違いしやすいケースとして，cv::Mat
    mat=cv::Mat::zeros(3,4,CV_8UC1);std::cout\<\<mat.size()\<\<std::endl;
    とすると，
    -   \>\> 4,3 と表示されるといった現象があります．
    -   cv::Mat::size()は画像としてのサイズを返すので横x縦で帰ってきます．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    cv::Mat1f m = (cv::Mat1f(2, 2) << 0, 1, 2, 3);
    std::cout << "m=\n"  << m << std::endl; // [1,2;3,4]
    std::cout << "\nm(0,1) = " << m(0, 1) << std::endl; // =1
    std::cout << "m(cv::Point2i(0,1)) = " << m(cv::Point2i(0, 1)) << std::endl; // =2
}
```

`cv::imshow()は後ろでcv::waitKey()を呼ばないと画像が表示されない`

-   待ち時間とかいらないから，ずっと画像を流してほしいと思ってもcv::waitKeyが必要になるのです．
-   たしかPythonでもそうだったと思います．
-   ところで，cv::waitKey()は引数で待ち時間\[ms\]を指定できるのですが，OpenCV3以前ではその待ち時間の精度がたかだか15\[ms\]程度しか無いらしいです．
-   [この記事](https://qiita.com/tomoaki_teshima/items/8f20e8c65e3568f6060f)が参考になります

`原点は原則左上，左->右がX軸方向，上->下がY軸方向`

-   当然ですがZ軸は画面奥向きになります．
-   Z軸の向きとX軸の向きをそういうふうにすると，Y軸は自然と下をむくので仕方がありません．
-   つまづきやすいポイントです．
-   cv::line(img, cv::Point(0, 0), cv::Point(img.size()),
    cv::Scalar::all(255)); は左上から右下に線が引かれますよ．

### cv::Mat

-   OpenCVで画像を扱うときにもっぱら扱う基本的な型です．
-   中身はn次元の密な配列です．
-   配列なので，アドレスは連続ですよ．
-   実数や複素数，ベクトル，グレースケール画像，カラー画像，その他いろいろをもたせることができます．
-   ところで，ご存知かもしれませんがデジタル画像は各画素の輝度(など)をデジタル値で持ってます．
-   代表的なデジタル画像のフォーマットにJPEGがありますが，これは具体的に1チャンネルあたり8bit正整数で画素の値が表現されています．
-   つまり，各画素で各色について0\~255の値しかとりようがありません．
-   cv::Matはそれにとらわれず，いろいろ指定することができます．
-   

#### 構成

-   基本的な構成要素は以下になります．
    -   行数(col)
    -   列数(row)
    -   型(type)
    -   チャンネル(channels)
-   初期化するときはこれらを指定してやります．
-   なお，画像としてcv::imshow()で描画できるのは1また3チャンネルの符号なし8bitまたは符号なし16bitだけです．

#### 初期化

-   沢山あるので，都合に応じて使い分けられるようにしましょう．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <iostream>
#include <opencv2/core.hpp>
int main()
{
    // 全要素が(0,0,0)の(6x4)の符号なし8bitのRGB画像(?)です．
    cv::Mat image1 = cv::Mat::zeros(4, 6, CV_8UC3);
    std::cout << "\n image1\n"
              << image1 << std::endl;

    // 全要素が(1)の(6x4)のグレースケール画像です．
    cv::Mat image2 = cv::Mat::ones(4, 6, CV_8UC1);
    std::cout << "\n image2\n"
              << image2 << std::endl;

    // 実態はありません．
    cv::Mat image3;
     std::cout << "\n image3\n" << image3 << std::endl;

    // 1+3j で埋められた 7x7 の浮動小数点複素行列を作成します．
    cv::Mat image4 = cv::Mat(7, 7, CV_32FC2, cv::Scalar(1, 3));
    std::cout << "\n image4\n"
              << image4 << std::endl;

    // 3x3の浮動小数点で(1 2 3; 4 5 6; 7 8 9)みたいになります．
    double data5[3][3] = {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}};
    cv::Mat image5 = cv::Mat(3, 3, CV_64F, data5);
    std::cout << "\n image5\n"
              << image5 << std::endl;

    // 対角要素が(1,0,0)の4x4です．
    cv::Mat image6 = cv::Mat::eye(4, 4, CV_8UC3);
    std::cout << "\n image6\n"
              << image6 << std::endl;

    // 3x3 の倍精度単位行列を作成します．
    cv::Mat image7 = (cv::Mat_<double>(3, 3) << 1, 0, 0, 0, 1, 0, 0, 0, 1);
    std::cout << "\n image7\n"
              << image7 << std::endl;

    // 3x3 の黄色の画像を作成します．
    cv::Mat image8 = cv::Mat(3, 3, CV_8UC3, cv::Scalar(0, 255, 255));
    std::cout << "\n image8\n"
              << image8 << std::endl;
}
```

-   Pythonの場合はcv::Matの代わりにnumpy.arrayを使用します．
-   本質的じゃないので，軽く流します．
-   詳しくはnumpyの[解説ページ](https://www.google.com/)を参考にしてください．

```{=html}
<div align="center">
```
main.py

```{=html}
</div>
```
``` {.python .numberLines}
import cv2
import numpy as np

# 上半分が青で下半分が赤です． 
image=np.zeros((480,640,3),np.uint8)
image[:240,:,0]=255;
image[240:,:,2]=255
  
cv2.imshow("image",image)
cv2.waitKey(0)
```

#### 算術演算

-   [探せ!この世の全てをそこに置いてきた．](https://docs.opencv.org/3.2.0/d3/d63/classcv_1_1Mat.html#af1d014cecd1510cdf580bf2ed7e5aafc)

#### 要素アクセス

-   いくつか方法があります．
-   全て走査するなら，for_eachですね．
-   そうでもないならポインタで，見やすさならcv::Mat.at`<Type>`{=html}かもしれません．
-   参考になる[記事](https://qiita.com/dandelion1124/items/94542d8cd7b3455e82a0)です．ここからサンプルを引用します．
-   以下はVGA画像を走査して黒い画像を，赤い画像に書き換えることをやっています．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
    cv::Mat src = cv::Mat::zeros(480,640, CV_8UC3);
    // atを用いた愚直な方法
   for (int y = 0; y < src.rows; y++) {
        for (int x = 0; x < src.cols; x++) {
            src.at<cv::Vec3b>(y, x)[0] = 0;
            src.at<cv::Vec3b>(y, x)[1] = 0;
            src.at<cv::Vec3b>(y, x)[2] = 255;
        }
    }
    // iteratorを用いた方法
    cv::MatIterator_<cv::Vec3b> itr = src.begin<cv::Vec3b>();
    cv::MatIterator_<cv::Vec3b> itr_end = src.end<cv::Vec3b>();
    for (int i = 0; itr != itr_end; itr++, i++) {
        cv::Vec3b bgr = (*itr);
        (*itr)[0] = 0;
        (*itr)[1] = 0;
        (*itr)[2] = 255;
    }
    // pointerを用いた方法
    for (int y = 0; y < src.rows; y++) {
        cv::Vec3b* p = &src.at<cv::Vec3b>(y, 0);
        for (int x = 0; x < src.cols; x++) {
            (*p)[0] = 0;
            (*p)[1] = 0;
            (*p)[2] = 255;
            p++;
        }
    }
    // pointerを用いた方法2
    for (int y = 0; y < src.rows; y++) {
        cv::Vec3b* p = src.ptr<cv::Vec3b>(y, 0);
        for (int x = 0; x < src.cols; x++) {
            (*p)[0] = 0;
            (*p)[1] = 0;
            (*p)[2] = 255;
            p++;
        }
    }
    // forEachを持ちいた方法
    src.forEach<cv::Vec3b>(
        [](cv::Vec3b& p, const int position[2]) -> void {
            p[0] = 0;
            p[1] = 0;
            p[2] = 255;
            // col=position[0],row=position[1]で座標が得られる
        });
```

-   ところで，先の記事にある実行速度テストを試したところ僕の環境では以下のようになりました．

`[XGA]`\
`at: 905.325[ms]`\
`iterator: 2666[ms]`\
`pointer: 424.506[ms]`\
`pointer2: 427.706[ms]`\
`forEach: 205.349[ms]`\
\
`[720p]`\
`at: 1081.64[ms]`\
`iterator: 3134.24[ms]`\
`pointer: 494.841[ms]`\
`pointer2: 497.596[ms]`\
`forEach: 225.591[ms]`\
\
`[1080p]`\
`at: 2383.15[ms]`\
`iterator: 7045.27[ms]`\
`pointer: 1111.49[ms]`\
`pointer2: 1113.82[ms]`\
`forEach: 495.965[ms]`

-   forEachが圧倒的に速いですね．
-   結構差があるので，気をつけてください．

#### コピー・分解・結合・変形

-   サンプルプログラムとしてできるだけ詰め込みました．
-   色々な画像を生成したり，くっつけたり分解したりして最後に合体した画像(300x300)を出力します．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
#include <vector>
int main()
{
    cv::Mat image1 = cv::Mat(100, 100, CV_8UC3, cv::Scalar(0, 255, 255));  // 黄色い画像
    cv::Mat image2 = image1;                                               // 浅いコピー(同じものを差す)
    cv::Mat image3 = image1.clone();                                       // 深いコピー(同じものを生成して代入する)
    cv::circle(image1, cv::Point(50, 50), 10, cv::Scalar::all(0), -1);     // image1に円を描画(image2にも同様の操作がなされる)

    cv::Mat image4 = cv::Mat(100, 100, CV_8UC3, cv::Scalar(0, 255, 0));                                    // 緑の画像
    cv::putText(image4, "HOGE", cv::Point(10, 50), cv::FONT_HERSHEY_PLAIN, 1.5, cv::Scalar::all(255), 2);  // 文字の描画
    cv::resize(image4, image4, cv::Size(300, 100));                                                        // 300x100に変形(cv::Sizeの引数の順番に注意)
    cv::flip(image4, image4, 0);                                                                           // 画像の反転(0:上下反転,正:左右反転,負:両方反転)

    cv::Mat image5 = cv::Mat(100, 100, CV_8UC3, CV_RGB(0, 0, 0));                 // 黒い画像
    cv::Mat image6;                                                               // 無
    cv::rectangle(image5, cv::Rect(10, 20, 30, 40), cv::Scalar(0, 255, 255), 3);  // 黄色い長方形を描画
    cv::repeat(image5, 1, 2, image6);                                             // 水平方向に2回繰り返し(引数の順番に注意)
    image5 = image4.colRange(0, 100);                                             // 画像の特定の範囲(左上から100列分)だけ取り出し

    cv::Mat image7;                                                     // 無
    cv::hconcat(std::vector<cv::Mat>{image1, image2, image3}, image7);  // 水平方向の結合 (1:2:3->7)
    cv::hconcat(image5, image6, image6);                                // 水平方向の結合 (5:6->6)
    cv::vconcat(std::vector<cv::Mat>{image7, image4, image6}, image7);  // 垂直方向の結合

    cv::namedWindow("window", cv::WINDOW_NORMAL);
    cv::imshow("window", image7);
    cv::waitKey(0);  // 指定秒[ms]の間キーボード入力を待つ(0以下なら押されるまで待つ)
}
```

-   以下に実行したときの出力画像を示します．
    -   コピーされているところ，されていないところ，どのように結合しているかを確認してください．

![](1546283350618-Screenshot_from_2019-01-01_04-08-26.png "1546283350618-Screenshot_from_2019-01-01_04-08-26.png"){width="600"}

#### cv::UMat・cv::GPUMat

-   これはちょっとテクニカルです．
-   cv::Matの強いやつです．
-   計算が早くなります．
-   cv::GPUMatは名前にもあるように，GPUで処理してくれます．
    -   ただし使用するにはCUDAが必須です．
-   cv::UMatは何の略なのかわかりませんが，コレを使っておくとOpenCLが使える環境なら自動でOpenCLを使ってくれるそうです．
    -   さもなければcv::Matと同じ挙動になるらしいです．
-   どちらも僕が殆ど使ったことがないので，こういうのがあるよという紹介で済ませてしまいます．

#### 練習 {#練習_1}

-   unsigned
    charの深さを持ったVGAサイズの画像で左半分を赤，右半分を青であるようなものを表示せよ．

```{=mediawiki}
{{Collapse_top|例}}
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    cv::Mat image = cv::Mat(480, 640, CV_8UC3, cv::Scalar(0, 0, 255));
    cv::Mat(480, 320, CV_8UC3, cv::Scalar(255, 0, 0)).copyTo(image.colRange(0, 320));
    cv::imshow("window", image);
    cv::waitKey(0);
}
```

``` {.python .numberLines}
import cv2
import numpy as np
img = np.zeros((480, 640, 3), np.uint8)
img[:, :320, 2] = 255
img[:, 320:, 0] = 255
cv2.imshow("image", img)
cv2.waitKey(0)
```

```{=mediawiki}
{{Collapse_bottom}}
```
### その他の基本的な型

-   面倒なので雑にコメントして流します．

#### cv::Scalar

-   スカラーという名前のくせに，4つの要素をもったベクトルです．
-   BGRを指定する時以外に使うシチュエーションを知りませんが，BGRを指定するときは必ず登場します．
-   4つめの要素に0以外を代入したことがありません．
-   ちなみにスカラーという名前なのですがノルムを計算することができます．

`cv::norm(cv::Scalar(3,4,0)); // double型の5`

#### cv::Point\_ , cv::Point3\_ {#cvpoint__cvpoint3_}

-   2次元や3次元の空間の点を表すときに使えます．加減乗と内積が定義されてます．
-   以下のような別名が定義されてます．
-   他の型もだいたいこんな感じsuffixで型と次元が表す別名が定義されてます．

`typedef Point_``<int>`{=html}` Point2i;`\
`typedef Point_``<int64>`{=html}` Point2l;`\
`typedef Point_``<float>`{=html}` Point2f;`\
`typedef Point_``<double>`{=html}` Point2d;`\
`typedef Point2i Point;`\
`typedef Point3_``<int>`{=html}` Point3i;`\
`typedef Point3_``<float>`{=html}` Point3f;`\
`typedef Point3_``<double>`{=html}` Point3d;`

#### cv::Vec

-   template\<typename \_Tp, int cn\> class Vec
-   cv::Point_と被りすぎて使い分けがよくわからないです．
-   これを使うならcv::Matで列数を1にしたほうが見通しがいいです．

#### cv::TermCriteria

-   反復アルゴリズムの停止基準でして，初見で戸惑うのですが，大したことはなくて，
    反復を`<反復回数>`{=html}で切るか，`<要求精度>`{=html}で切るか，その両方かを指定するだけのものです．
-   以下はOpenCVのソースから引っ張ってきました．

```{=html}
<div align="center">
```
include/core/types.hpp

```{=html}
</div>
```
``` {.python .numberLines}
class CV_EXPORTS TermCriteria
{
public:
    /**
      Criteria type, can be one of: COUNT, EPS or COUNT + EPS
    */
    enum Type {
        COUNT = 1,         //!< the maximum number of iterations or elements to compute
        MAX_ITER = COUNT,  //!< ditto
        EPS = 2            //!< the desired accuracy or change in parameters at which the iterative algorithm stops
    };

    //! default constructor
    TermCriteria();
    TermCriteria(int type, int maxCount, double epsilon);

    inline bool isValid() const
    {
        const bool isCount = (type & COUNT) && maxCount > 0;
        const bool isEps = (type & EPS) && !cvIsNaN(epsilon);
        return isCount || isEps;
    }

    int type;        //!< the type of termination criteria: COUNT, EPS or COUNT + EPS
    int maxCount;    //!< the maximum number of iterations/elements
    double epsilon;  //!< the desired accuracy
};
```

### 入出力・表示

#### imshow

-   画像を描画できます．
-   単品でも使えますが，cv::namedWindowと組み合わせると素敵にできます．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    cv::Mat image = cv::Mat::zeros(480, 640, CV_8UC1);
    // アスペクト比を一定にして可変なウィンドウが表示される
    cv::namedWindow("window1", cv::WINDOW_NORMAL | cv::WINDOW_KEEPRATIO);
    cv::imshow("window1", image);
    cv::waitKey(0);

    // 画像ピッタリサイズのウィンドウが表示される(大きさは固定)
    cv::namedWindow("window2", cv::WINDOW_AUTOSIZE);
    cv::imshow("window2", image);
    cv::waitKey(0);

    // フルスクリーン
    cv::namedWindow("window3", cv::WINDOW_NORMAL);
    cv::setWindowProperty("window3", cv::WND_PROP_FULLSCREEN, cv::WINDOW_FULLSCREEN);
    cv::imshow("window3", image);
    cv::waitKey(0);
}
```

-   画像に線やら円を描画したいときは
    cv::Rectangle,cv::circle,cv::lineなどが使えます．
-   文字を描画したいときはcv::putTextを用います．
    -   なおOpenCVで利用できるフォントはどれも微妙な見た目です．
    -   あと，日本語を表示させたいときはフォントのあるパスとかをしていしたりすると表示できるようになるようです．
    -   以下に標準で使えるフォントが羅列されるサンプルを示します．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    cv::Mat img = cv::Mat::zeros(500, 500, CV_8UC3);
    int face[] = {cv::FONT_HERSHEY_SIMPLEX, cv::FONT_HERSHEY_PLAIN, cv::FONT_HERSHEY_DUPLEX, cv::FONT_HERSHEY_COMPLEX,
        cv::FONT_HERSHEY_TRIPLEX, cv::FONT_HERSHEY_COMPLEX_SMALL, cv::FONT_HERSHEY_SCRIPT_SIMPLEX,
        cv::FONT_HERSHEY_SCRIPT_COMPLEX, cv::FONT_ITALIC};

    // 画像，テキスト，位置（左下），フォント，スケール，色，線太さ，種類
    cv::putText(img, "OpenCV", cv::Point(50, 50), face[0], 1.2, cv::Scalar(0, 0, 200), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(50, 100), face[1], 1.2, cv::Scalar(0, 200, 0), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(50, 150), face[2], 1.2, cv::Scalar(200, 0, 0), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(50, 200), face[3], 1.2, cv::Scalar(0, 100, 100), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(50, 250), face[4], 1.2, cv::Scalar(100, 100, 0), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(50, 300), face[5], 1.2, cv::Scalar(100, 0, 100), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(50, 350), face[6], 1.2, cv::Scalar(100, 100, 100), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(50, 400), face[7], 1.2, cv::Scalar(100, 100, 200), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(300, 50), face[0] | face[8], 1.2, cv::Scalar(100, 200, 100), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(300, 100), face[1] | face[8], 1.2, cv::Scalar(200, 100, 100), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(300, 150), face[2] | face[8], 1.2, cv::Scalar(200, 200, 100), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(300, 200), face[3] | face[8], 1.2, cv::Scalar(200, 100, 200), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(300, 250), face[4] | face[8], 1.2, cv::Scalar(100, 200, 200), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(300, 300), face[5] | face[8], 1.2, cv::Scalar(100, 200, 255), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(300, 350), face[6] | face[8], 1.2, cv::Scalar(100, 255, 200), 2, cv::LINE_AA);
    cv::putText(img, "OpenCV", cv::Point(300, 400), face[7] | face[8], 1.2, cv::Scalar(255, 200, 100), 2, cv::LINE_AA);
    cv::imshow("window", img);
    cv::waitKey(0);
}
```

![](1546287384269-Screenshot_from_2019-01-01_05-15-50.png "1546287384269-Screenshot_from_2019-01-01_05-15-50.png"){width="500"}

##### addText

-   どのフォントも全体的にダサいのですが，OpenCVをbuildする際に，WTH_QTをONにしていた場合，cv::addTextなるAPIを利用できます．
-   これでフォントにConsolasを採用するとかなり見た目が素敵になります．
-   日本語に対応したフォントを採用すれば日本語もいけます．
    -   **ただし，絵文字はRGBがBGRになります．**
    -   **cv::namedWindowを呼んでから出ないと使えません．**

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    cv::Mat img = cv::Mat::zeros(500, 500, CV_8UC3);
    cv::namedWindow("window", 0);
    cv::addText(img, "＋＞＜，．・￥；：」＠「ー￥＿", cv::Point(50, 100), "Consolas", 20, CV_RGB(255, 0, 0));
    cv::addText(img, "！”＃＄％＆’（）〜＝｜｛｝｀＊？", cv::Point(50, 200), "Consolas", 20, CV_RGB(255, 0, 0));
    cv::addText(img, "籔", cv::Point(50, 350), "Consolas", 100, CV_RGB(255, 0, 0));
    cv::addText(img, "🗾", cv::Point(200, 350), "Consolas", 100, CV_RGB(255, 0, 0));
    cv::addText(img, "🐶", cv::Point(350, 350), "Consolas", 100, CV_RGB(255, 0, 0));
    cv::imshow("window", img);
    cv::waitKey(0);
}
```

![](1546613175568-Screenshot_from_2019-01-04_23-43-38.png "1546613175568-Screenshot_from_2019-01-04_23-43-38.png"){width="500"}

#### imwrite

-   画像を保存できます
-   色々引数に詰め込むとエンコードのパラメータとか変えられるらしいです．
    -   やったことないです．
-   ディレクトリを新しく作ったりはしてくれませんので注意してください．
-   以上です．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    cv::Mat image = cv::Mat::zeros(480, 640, CV_8UC1);
    cv::imwrite("image.png", image);
}
```

#### imread

-   画像を読み込みます．
-   対応している書式は以下の通りです．

  --------------------------- --------------------------------------- ----------
  書式                        拡張子                                  サポート
  Windows bitmaps             \*.bmp, \*.dib                          always
  Portable image format       \*.pbm, \*.pgm, \*.ppm \*.pxm, \*.pnm   always
  Sun rasters                 \*.sr, \*.ras                           always
  Radiance HDR                \*.hdr, \*.pic                          always
  JPEG files                  \*.jpeg, \*.jpg, \*.jpe                 
  JPEG 2000 files             \*.jp2                                  
  Portable Network Graphics   \*.png                                  
  WebP                        \*.webp                                 
  TIFF files                  \*.tiff, \*.tif                         
  OpenEXR Image files         \*.exr                                  
                                                                      
  --------------------------- --------------------------------------- ----------

-   我々がよく扱うのはpng,jpeg,bmpぐらいですが，困ることはありません．
-   以下にサンプルを示します．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    cv::Mat image = cv::imread("画像.png");  // 実行ファイルのあるディレクトリにある"画像.png"を読み込む
    cv::imshow("window", image);
    cv::waitKey(-1);
    image = cv::imread("画像.png", cv::IMREAD_GRAYSCALE);  // グレースケールで読み込む
    cv::imshow("window", image); // 色相を失った画像が表示される
    cv::waitKey(0);
}
```

-   画像がなければ，空のcv::Matが返ってきます．

#### videoCapture

-   カメラから画像を取り出したり，動画から画像を取り出したりします．
-   まずはwebカメラから画像を取り出す例です．
    -   最近は殆どのノートパソコンにフロントカメラがついているので，それから画像を取得すると思います．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include "opencv2/opencv.hpp"
int main(int argh, char* argv[])
{
    cv::VideoCapture cap(0);  // デバイスのオープン(複数個のカメラがあるならば1やら2やらを指定する)
    //cv::VideoCapture cap; cap.open(0);// こっちでもOK

    if (!cap.isOpened())  // カメラデバイスが正常にオープンしたか確認
        return -1;

    cv::Mat image;
    while (true) {
        cap >> image;  // カメラから画像の取得
        cv::imshow("window", image);

        int key = cv::waitKey(1);
        if (key == 'q')  // qボタンが押されたとき
            break;
        if (key == 's')  // sが押されたとき
            cv::imwrite("img.png", image);
    }
    return 0;
}
```

-   カメラではなくて動画を読み込んで処理がしたい場合は先のコードで4,5行目を以下のようにすると動画が読み込めます．他は同じです．

``` {.cpp .numberLines}
 cv::VideoCapture cap("千本桜.mp4");  // 動画のオープン
 //cv::VideoCapture cap; cap.open("千本桜.mp4");  // こっちでもOK
```

-   またcv::videoCaptureのget,setメソッドを使うことで，動画やカメラのパラメータを確認したり，変更したりすることができます．

``` {.cpp .numberLines}
// 幅
 int W = video.get(cv::CAP_PROP_FRAME_WIDTH);
 // 高さ
 int H = video.get(cv::CAP_PROP_FRAME_HEIGHT);
 // 総フレーム数
 int count = video.get(cv::CAP_PROP_FRAME_COUNT);
 // fps
 double fps = video.get(cv::CAP_PROP_FPS);
```

-   これを用いればカメラのFPSや露光時間を変更することができます．
-   [ここ](https://docs.opencv.org/3.2.0/d4/d15/group__videoio__flags__base.html#gaeb8dd9c89c10a5c63c139bf7c4f5704d)にどんなパラメータをいじれるかのリストがあります．
-   ところで，V4L2 test
    なるアプリがubuntuのソフトウェアセンタからインストールが可能でして，
    -   これを使うと，GUIでカメラのパラメータやらをリアルタイムに変更できて素敵です．
    -   何かとはかどります．

![](1546289564287-Screenshot_from_2019-01-01_05-52-24.png "1546289564287-Screenshot_from_2019-01-01_05-52-24.png"){width="500"}

-   以下はその操作パネルです．

![](1546289486746-Screenshot_from_2019-01-01_05-49-54.png "1546289486746-Screenshot_from_2019-01-01_05-49-54.png"){width="600"}

#### videoWriter

-   動画(厳密には画像の集合を時系列順にしたもの)を出力することができます．
-   なお音声は扱うことはできません．
    -   無音の動画のみを生成することができます．
-   gifはつくれないです．
-   以下は30fpsの雑な動画を出力するサンプルプログラムです．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    cv::VideoWriter writer("video.avi", cv::VideoWriter::fourcc('M', 'J', 'P', 'G'), 30, cv::Size(640, 480));
    if (not writer.isOpened())
        return -1;

    cv::Mat image = cv::Mat::zeros(480, 640, CV_8UC3);
    for (int i = 0; i < 300; i++) {
        cv::circle(image, cv::Point(320, 240), i % 50, cv::Scalar::all(i % 100), -1);
        writer << image;
    }

    return 0;
}
```

-   cv::VideoWriter::fourccですが，後ろに並ぶ文字はエンコードの形式を指定しています．
-   何を書けばいいのかは保存したい形式によりますので適宜調べましょう．

#### viz

-   3次元のビューアを作ることができます．
-   RGBDセンサや3D-LRFとかは画像だけでは表現しきれないので，コレを使いたくなると思います．

`OpenCV 3までは本体の方に含まれていたのですが，OpenCV4からcontribの方に移動してしまったので，OpenCV4でこれを使いたいときはcontribもビルドする必要があります．`

![](1513618353586-Screenshot_from_2017-12-19_02-30-27.png "1513618353586-Screenshot_from_2017-12-19_02-30-27.png"){width="700"}

-   この画像ではXtion Pro Live
    で取得した点群を再投影していたのだと記憶しています．
    -   マウスを使って，視点変更などができます．
-   色々長くなるので，[ここ](https://docs.opencv.org/3.2.0/d7/df9/tutorial_table_of_content_viz.html)で使い方を調べてください．
-   3次元には点群だけでなく，直方体や球，矢印などを配置することができます．
-   当然アニメーションが可能です．
-   keyboardイベントやmouseイベントを登録できるようです．

#### depth画像の扱い

-   特に王道もないのですが，
-   基本的にはCV_32FC1で\[m\]単位でもっておいて，
-   保存するときはCV_16UC1で1\[m\]を5000になるようにして，
    -   (0.2mm刻みで最大13mを記録できて，頑張ると読める)
    -   visual-SLAMに強い大学[TUM](https://vision.in.tum.de/data/datasets)のデータ・セットがこれとおなじ
-   描画するときはHSVで色相を距離に比例させると素敵
-   画像読み込みをcv::Mat image = cv::imread(file_path,
    cv::IMREAD_ANYDEPTH);のようにするとよい．

```{=html}
<!-- -->
```
-   上の書式で保存した画像を置いておきます．
    -   頑張って画像を見るとわかりますが，距離30cmぐらいのところにある球体が消失しています．
    -   なにか遊んでみるといいかもしれません．(例えば，球の中心座標を計測するだとか)

![](1561701465302-depth1.png "1561701465302-depth1.png"){width="400"}
![](1561701465312-depth2.png "1561701465312-depth2.png"){width="400"}

#### 練習 {#練習_2}

-   webカメラからの映像を取得して，ウィンドウに描画しつつ，動画ファイルに保存せよ．

### 基本的な処理

-   [ここ](https://docs.opencv.org/3.2.0/d7/da8/tutorial_table_of_content_imgproc.html)を見ると良いです．
    -   平滑化
    -   膨張収縮
    -   モルフォロジー変換
    -   閾値処理
    -   エッジ検出
    -   輪郭検出
    -   Hough変換
-   とかがあります．

#### 練習 {#練習_3}

-   適当な画像から赤っぽいところだけを残し，他を黒にする実装を書いてください．
-   簡単に実装するなら，cv::cvtColorでBGR画像をHSVに変換して，cv::thresholdで色相が特定の範囲外なら0にするみたいなことをするとできると思います．
-   あとは[こういうこと](https://docs.opencv.org/3.2.0/d0/d49/tutorial_moments.html)をすると赤っぽい領域の重心とかが得られます．

![](1546194675388-opencv.png "1546194675388-opencv.png"){width="400"}
`{{Collapse_top|例}}`{=mediawiki}

``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
// huga
}
```

```{=mediawiki}
{{Collapse_bottom}}
```
### sample-program紹介 {#sample_program紹介}

-   サンプルプログラム多すぎ
-   以上です．

![](1546198379947-Screenshot_from_2018-12-31_04-32-23.png "1546198379947-Screenshot_from_2018-12-31_04-32-23.png"){width="600"}

[`おすすめの楽曲`](https://www.youtube.com/watch?v=2kZVEUGLgy4)`です`

## デバイス

-   RoboTechで使えるセンサを幾つか紹介します．
-   センサの性能とライブラリ周りを説明できたらいいかなと思います．
-   センサについては[
    こちら](RC2018/Workshop2018/Intel_Realsense、Xtion2の調査 "wikilink")も参考にするとよいと思います．
    -   というかまとまったページないのか・・・
-   ここで上げるセンサの多くはDepth(深度)を計測することができます．
-   つまり色情報だけでなく距離情報も利用可能になるのです．
-   深度を利用できるようになると途端にできることが増えますので，有効活用していきましょう．
-   なお僕がメインだったRC18ではロボットにこういうセンサが載ることはありませんでした．
-   深度を取る方法にはPassive(受動的)とActive(能動的)の２つに分けることができます．
    -   Passiveとは環境に干渉することなく非侵襲的に距離を計測することです．
        -   具体的にはStereoCameraのような複数台のカメラを用いて，視差を使って距離を計測します．
        -   これは結局色情報に依存しているので，光の無いところでは利用できませんし，逆に明るすぎるようなところでは(露光時間を短くするなり感度を下げるなりしない限り)利用できません．
        -   あとテクスチャのない環境でも利用することができません．
            -   左カメラで見ている点が，右のカメラのどこに対応するかがわからないためです．
        -   人間が目視で利用可能か否かが判断できることが多いのが利点です．Activeの場合赤外線を使うこと多いですが，人類は空間を漂う赤外線を認識することができないのでActiveセンサがじつは使えないといったことがあります．
    -   Activeとは対照的に環境に対して光を投影したりすることで距離を測定する方法です．
        -   例えばToF(Time of Flight)方式というものがあります．
            -   これは光(レーザー)を照射して，対象物で反射した光が戻ってくるまでの時間を測ることで距離を測定します．
            -   レーザーのエネルギー強度や波長にもよりますが，かなり外乱に強いです．
        -   他にもStructured-Light方式というものもあります．
            -   プロジェクタを用いて特殊な幾何パターンを環境に対して投影し，そのパターンがどのようにカメラに映るかをみることで環境構造を測定し，距離を算出する方法です．
            -   多くの場合，赤外線で幾何パターンを投影します．
            -   投影する光の波長にもよりますが，ToFに比べて外乱光に弱いです．
-   僕が2年秋のロボゼミで発表したスライドがこの章の参考になるかもしれません．
    -   可視光・紫外線・赤外線・色・人間の視覚・照明などに触れられています．

![](kyabuuchi-robosemi.pdf "kyabuuchi-robosemi.pdf")

-   ところで昔は[RC2014/制御屋/USBデバイス講習](RC2014/制御屋/USBデバイス講習 "wikilink")があったらしいですね．

### 選定

-   RGBDセンサは色々あって選定が大変ですね．
-   以下の項目に注目するとよいと思います．

  ----------------------------------- ------------------------------------------------ -------------------------------------------------------------------------------------------------------------------------------
  項目                                例                                               解説
  Wave Length(StructuredLightやToF)   850nm(realsense)                                 波長は可視光から遠いほど照明の外乱光に対して強くなります．(照明は本来可視光を出すものなので，可視光から離れるほど弱い)
  Depth Range                         30m(UTM30-LX)                                    LRF以外なら高々7m程度な気がしますね．
  Resolution                          640x480                                          RGBとDで解像度が異なることが多いです．
  Frame Rate                          30fps                                            解像度とfpsが組になっていて，最大解像度だと最大fpsが出ないといった商品も多いです．(最大fpsという表記の意味に注意しましょう．)
  Angle(Horizontal,Vertical)          H:70deg,V:60de (kinect-v2)                       広い範囲を見渡せられると良いですね．(垂直画角と水平画角は多くの場合異なります．)
  Requirements                        Nvidia GPU with compute capability \> 3.0(ZED)   「GPUが必要です」と言われましても．
  Price                               60万円(YVT35-LX)                                 RGBDセンサは2\~3万程度ですね．
  Embedded IMU                        800Hz Gyroscope, Accelerometer(ZED mini)         あれば色々はかどります．
  Power                               USB 2.0 bus power (Xtion Pro Live)               この手の商品はだいたい12\~24Vで動きます．
  Connector & Protocol                Ethernet TCP/IP(YVT-35LX)                        だいたいUSB
                                                                                       
  ----------------------------------- ------------------------------------------------ -------------------------------------------------------------------------------------------------------------------------------

### kinect v1,v2 {#kinect_v1v2}

![](1546467458743-DSC_1141.jpg "1546467458743-DSC_1141.jpg"){width="400"}![](1546467458757-DSC_1142.jpg "1546467458757-DSC_1142.jpg"){width="400"}

-   Microsoftの商品です．
    -   XBOXを売るために生産したようです．
-   RGBDセンサです．30fpsでだいたいVGAサイズのRGBDデータを取得できます．
-   version1とversion2があります．
-   測距の方法がバージョンによって異なり，v1ならstructed-light方式，v2ならToF方式です．
    -   ToFのがいいです．
-   どちらも生産中止しているので，市場から消えれば消えます．
-   v2のほうが性能はいいですが，要求スペックが高くて，付属のアダプタがやたら重いです．
-   どちらも1万円ぐらいですね．
    -   ただしkinect-v2はPCと接続するアダプタが品薄で，しかも1万円するクソ仕様です．

  -------------- ---------------------- ----------------------------------------
  オプション     v1                     v2
  色 解像度      640x480                1900x1080
  色 fps         30fps                  30fps(環境光が暗いと自動で15fpsにする)
  深度 解像度    320x240                512x424
  深度 fps       30fps                  30fps
  人物領域       6人                    6人
  人物姿勢       2人                    6人
  関節           20関節/人              25関節/人
  手の開閉状態   △(Developer Toolkit)   ◯(SDK)
  深度のレンジ   0.8\~4.0m              0.5\~8.0m
  人物のレンジ   0.8\~4.0m              0.5\~4.5m
  水平視野角     57度                   70度
  垂直視野角     43度                   60度
  チルトモータ   ◯                      ×
  複数アプリ     ×                      ◯
                                        
  -------------- ---------------------- ----------------------------------------

-   動作要件

  ---------- ------------------- ----------------------
  項目       v1                  v2
  接続端子   USB2.0              USB3.0
  CPU        Dual-Core 2.666Hz   Intel Core i7 3.1GHz
  GPU        DirectX9.0c         DirectX11.0
  RAM        2.0GB以上           4.0GB以上
                                 
  ---------- ------------------- ----------------------

#### 使い方

-   libfreenectというライブラリを用いると利用できます．
    -   [libfreenect](https://github.com/OpenKinect/libfreenect):
        kinect-v1用
    -   [libfreenect2](https://github.com/OpenKinect/libfreenect2):
        kinect-v2用
    -   readmeに書いてある内容に従うとインストールできます
-   [Obsoleteを自称するドキュメント](ドキュメント/制御/Kinect_v2 "wikilink")

#### 練習 {#練習_4}

-   libfreenectを利用して，kinect-v1,kinect-v2からColorとIRとDepthを取得してみよ
-   [rc2017/libfreenect2-test](https://git.robotech.t.u-tokyo.ac.jp/rc2017/libfreenect2-test)が参考になります．

### LiDAR

#### LiDARとは

-   [wiki/LiDAR](https://ja.wikipedia.org/wiki/LIDAR)
-   [wiki/光波測距儀](https://ja.wikipedia.org/wiki/%E5%85%89%E6%B3%A2%E6%B8%AC%E8%B7%9D%E5%84%80)
-   まとめるとToF距離計がぐるぐる回って測域できるセンサです．

#### 北陽電機

-   日本でLiDAR作ってる会社です．
-   RoboTechが所有しているLiDARは全てここの製品です．
-   とても高い商品なので，借りています．
-   貸与の窓口を担当している人が下の写真の嶋地直広(右)さんです．
    -   この写真は2009/03/24のものなので，2018年にお会いしたときは写真よりも少し老けて見えました．

![](1546368193077-simadi.jpg "1546368193077-simadi.jpg"){width="200"}

-   [ここ](https://sourceforge.net/p/urgnetwork/wiki/top_jp/)に使い方などのドキュメントがあります．
-   URG(あーじ)と読みます．

#### 概要 {#概要_1}

-   RoboTechには以下のLiDARがあります．
-   UTM-30LX(40万)

![](1546368350083-20170726182750_739.jpg "1546368350083-20170726182750_739.jpg"){width="200"}

-   2台あります
    -   1台は北陽電機様の借り物です．
    -   「井上駿之介」シールの貼ってあるほうがヤフオクで降ろされたものらしいです．

```{=html}
<!-- -->
```
-   YVT-35LX(60万)

![](1544272562929-DSC_1126.jpg "1544272562929-DSC_1126.jpg"){width="400"}

-   RC19に向けて，2018年12月に借りました．
-   3D点群が取得できます．
-   さらにIMUを内蔵しており，3軸加速度・3軸角速度を1000Hzで計測します．

```{=html}
<!-- -->
```
-   URG-04LX(15万)

![](1546468598368-DSC_1144.jpg "1546468598368-DSC_1144.jpg"){width="400"}

-   どこかで拾ってきたものらしいです．
-   電源ケーブルが無いので，僕がむりやりケーブルをつけて非れもない姿になっています．

#### 使い方 {#使い方_1}

-   2Dであれば[libcraft-detection](https://git.robotech.t.u-tokyo.ac.jp/workshop18/libcraft-detection)を用いると利用できます．
    -   素敵なサンプルがないのですが[RC18のManual
        Robot](https://git.robotech.t.u-tokyo.ac.jp/rc2018/manual/tree/master/pc)で使われています．
-   3Dであれば[urg3d-sample](https://git.robotech.t.u-tokyo.ac.jp/KYabuuchi/urg3d-sample)を参考にするとよいです．
-   どちらも既にlibflierに含まれているようです．

#### 練習 {#練習_5}

-   LiDARからデータを取得し，点群を描画してみよ．
    -   リアルタイムに描画できるのが好ましい．

### Xtion

![](1546467486560-DSC_1143.jpg "1546467486560-DSC_1143.jpg"){width="400"}

-   Structured-Light方式で距離を取るセンサです．
-   ASUS製です．
-   性能は[こちら](https://www.asus.com/jp/3D-Sensor/Xtion_PRO_LIVE/specifications/)
-   RoboTechはXtionProLiveとXtionProをそれぞれ2台ずつ，合計4台所有しています．
    -   XtionProはIR画像及びdepth画像，XtionProLiveはそれに加えてRGBも計測できます．
-   [ドキュメント/制御/RC2015/シャトルの軌道予測](ドキュメント/制御/RC2015/シャトルの軌道予測 "wikilink")とかが参考になると思います．

#### 使い方 {#使い方_2}

-   OpenNIで利用できるようになります．
    -   NIは Natural Interfaces の略です．
-   [ドキュメント/制御/OpenNI2](ドキュメント/制御/OpenNI2 "wikilink")というページがあります．
-   プログラミングが上手な方のやぶうちさんが作った[素敵なサンプル](https://git.robotech.t.u-tokyo.ac.jp/Control/samples-openni2)があります．

#### 練習 {#練習_6}

-   複数台のXtionからDepthデータを取得してみよ．

### ZED

![](1545965688720-DSC_1137.JPG "1545965688720-DSC_1137.JPG"){width="400"}

-   stereolabs製のステレオカメラです．
-   5万ぐらいです．
-   RGBのステレオカメラなので，いうて人間で観測できる環境なら距離を計測することができます．
-   ところがSDKで得られるdepthデータは精度がヤバイらしいです．(悪い意味で)
    -   [RC2017/並行開発/認識/作業日誌#精度に関してまとめた(多分重要)\--室岡](RC2017/並行開発/認識/作業日誌#精度に関してまとめた(多分重要)--室岡 "wikilink")などで述べられている．
    -   結論は[ここ](RC2017/並行開発/認識/作業日誌#ZED性能評価--室岡 "wikilink")に述べられている．
        -   depthはだめらしいですね．
-   メーカーの性能ページは[こちら](https://www.stereolabs.com/zed/)
-   一方でSDKに自己位置推定が用意されており，Visual-SLAMをしてくれるらしいです．
    -   こちらはかなりいい感じです．
-   しかし要求性能が少し高いのが難点です．

`Dual-core 2.3GHz or faster processor`\
`4GB RAM or mote`\
`Nvidia GPU with compute capability > 3.0`

-   特にGPUの要求が厄介で，おかげさまで既存のRoboTechの汎用なPCの中ではJetson一択になっています．
-   ところでIMUは搭載しておらず，pureなVisualSLAMをしています．
    -   ZED miniにはIMUがあって，データも得られるらしいです．

`距離推定誤差は，奥行きの2乗に比例，ベースラインに反比例，焦点距離に比例`

#### 使い方 {#使い方_3}

-   \[<https://www.stereolabs.com/docs/getting-started/installation/>　ここ\]の内容を追えば使えるようになります．

#### 練習 {#練習_7}

-   Depthの精度が(悪い意味で)ヤバイことを確認せよ．

```{=mediawiki}
{{Collapse_top|参考}}
```
-   やばそうなことがわかる図 \-- [ふつれ](服部未来 "wikilink")

-   1mぐらいのロボットが5m先の100mm立方オブジェクトを見ています．

-   ![](1546607449888-sample-robot-looking-cube.png "1546607449888-sample-robot-looking-cube.png"){width="400"}

-   これで2つのカメラに映る映像を想像し，そこにノイズが載ったことを考えればわかるでしょう．

-   ちゃんと画角を取りましょう．

-   [画角についての問題(創造情報学院試2010年夏)](https://web.archive.org/web/20151118065627/http://i-web.i.u-tokyo.ac.jp/edu/course/ci/pdf/2009_8_ci_istmajor_ja.pdf)

```{=mediawiki}
{{Collapse_bottom}}
```
-   自己位置推定の精度が(良い意味で)ヤバイことを確認せよ．

### RealSense D435 {#realsense_d435}

-   Structured-Lightで測距するセンサです．
-   Intel製の商品です．
-   たぶん一番コスパがいいです．
-   [realsense
    D435i](https://click.intel.com/intel-realsense-depth-camera-d435i-imu.html)が測距性能もよく，IMUも搭載しており素敵です．

#### 使い方 {#使い方_4}

-   [これ](https://github.com/IntelRealSense/librealsense)

#### 練習 {#練習_8}

-   使い方の詳細を書け．

### RealSense T265 {#realsense_t265}

![](1555165501041-DSC_1269.JPG "1555165501041-DSC_1269.JPG"){width="400"}

-   Intel曰く「Tracking Camera」らしく，SLAMをしてくれます．
-   2万円です．
-   カメラは魚眼のステレオです．
    -   カメラそれぞれ 30fps
    -   gyro 200fps
    -   Accel 62fps
    -   姿勢推定(6DoF) 200fps
-   ただし，特異な動きをさせると自己位置が吹き飛ぶことが指摘されています．
    -   [RC2019/MR2/二号機/作業日誌#2019/04/13](RC2019/MR2/二号機/作業日誌#2019/04/13 "wikilink")

#### 使い方 {#使い方_5}

-   [これ](https://github.com/IntelRealSense/librealsense)
-   付属しているrealsense-viewerで即座にSLAMを楽しめる

#### 練習 {#練習_9}

-   遊べ．

### web camera {#web_camera}

![](1546467728364-無題_1.png "1546467728364-無題_1.png"){width="400"}

-   USBやEthernet接続のカメラです．
-   いうて普通のカメラです．
-   ラズパイとかでも動くので，使いどころはあります．
-   ものによっては140fps出るUSBカメラとかもあります．

#### 使い方 {#使い方_6}

-   OpenCVのvideoCaptureを使えばいいと思います．

#### 練習 {#練習_10}

-   練習内容を考えよ．

## カメラ幾何

### ピンホールカメラと座標系

-   カメラで得られる像から3次元を復元するためには，逆に3次元の構造がどのようにしてカメラに映るからを知っている必要がある．
-   この章では，よく利用されるピンホールカメラモデルからスタートして射影行列$P$を求める．

```{=html}
<!-- -->
```
-   まず，カメラの全貌のモデルであるピンホールカメラモデルを考える．
-   このモデルは下図のようにピンホール(レンズ中心)を通る光線のみで結像を考えるモデルである．

![](1561430196896-Screenshot_from_2019-06-25_11-27-27.png "1561430196896-Screenshot_from_2019-06-25_11-27-27.png"){width="600"}

-   ピンホールカメラでは図の左側にあるような倒立した像が得られるのだが，扱いが面倒なので普通は右側にあるような仮想的な座標系で像を考える．
-   以降の話でも右側2つの座標系を扱う．

```{=html}
<!-- -->
```
-   次にピンホールカメラを含む全ての必要な座標系を示す．

![](1561430233536-Screenshot_from_2019-06-25_11-36-10.png "1561430233536-Screenshot_from_2019-06-25_11-36-10.png"){width="700"}

-   (図で$Z_c$だけ長い矢印で示されているが別に単位長さが違うわけではなく，ただただ同一直線状にあることを示しているだけである)

```{=html}
<!-- -->
```
-   多くの座標系があるので，座標を表記するときは下付き文字でどの座標系での表示かを示す.
-   以下に，各座標系の特徴を述べる．
-   **ワールド座標系**
    -   $O_w-X_wY_wZ_w$
    -   これは名前のとおりであり，撮影対象の座標などはこれで表記される.
    -   カメラのある座標もこれで表される．
-   **カメラ座標系**
    -   $O_c-X_cY_cZ_c$
    -   カメラを基準とした座標系である．
    -   原点は像の投影中心であり，図1でのピンホールに相当する．
    -   ワールド座標系でのカメラの座標が定まれば定義可能．
-   **正規画像座標系**
    -   $O_q-X_qY_q$
    -   カメラ座標系を仮想的な画像平面へ投影した座標系である．
    -   一般に原点は画像の中央になる．
    -   Z座標が正規化され，固定値になったものである．(具体的には1になる．)
-   **画像座標系**
    -   $O_i-X_iY_i$
    -   撮像素子の配置に依る変換をして，いわゆる画像に帰着された座標系である．
    -   これまでの座標系と異なり，単位がピクセルにすることが普通である．
    -   原点は画像の左下端になる．

```{=html}
<!-- -->
```
-   ここでの目的は世界座標で表記された座標$\vec{X}_W=(x_W,y_W,z_W)^{T}$から画像座標上の$\vec{X_i}=(x_i,y_i)$に変換する写像を得ることである．
-   非線形な変換が含まれるのだが，てきとうに近似して射影変換に落とし込む．

### 射影行列

座標系の橋渡しになる変換行列を順に求めていく．

#### ワールド座標系⇨カメラ座標系

-   この変換は3次元空間での並進運動と回転運動に切り分けられる．
-   並進運動はカメラの投影中心$O_c$からワールドの原点$O_w$へのベクトル$\vec{t}=\vec{O_cO_w}$で特徴付けられる．
-   また回転運動は，3次元空間の回転なので3x3の行列$R$で特徴付けられる．

```{=html}
<!-- -->
```
-   例えば，ワールド座標の点$\vec{X_W}=(x_W,y_W,z_W)^{T}$はカメラ座標では次のように表せられる．

$\vec{X_c}=R\vec{X_w}+\vec{t}$

-   この変換は次と同値である．

$\vec{X_c} =[R\ \ \vec{t}] 
 \left[\begin{matrix} \vec{X_w} \\ 1 \end{matrix}\right]$

**$\left[R\ \ \vec{t} \right]$`を(カメラの)外部パラメータと呼ぶ．`**

#### カメラ座標系⇨正規画像座標系

-   正規画像座標系はカメラ座標系を$Z_c=1$の平面に投影したものである．
-   ゆえに，次の変換で表せられる．

$\vec{X_q}=(x_q\ y_q\ 1)^{T}=(\frac{x_c}{z_c}\ \frac{y_c}{z_c}\ \frac{z_c}{z_c})^{T} =\frac{\vec{X_c}}{z_c}$

`ここで，`**`2つのベクトル`$\vec{x},\vec{y}$`の向きが等しいとき，両者に次の関係があると定義する．`**\
$\vec{x} \simeq \vec{y}$

-   するとカメラ座標系と正規画像座標系の関係は次のように書ける

$\vec{Xq}=\left(\begin{matrix}x_q\\y_q\\1\end{matrix}\right)\simeq\vec{X_c}$

-   この表現であっても$\vec{X_q}$は一意に定まっていることに注意されたい．

$\because \ \vec{X_c}$によって向きは定められており，$z_q=1$であるからスケールも定まっている．

#### 正規画像座標系⇨画像座標系

-   多くの場合，ここの変換で単位が\[pixel\]に変ることに注意されたい．
-   従来は実世界の広さを示すために長さの単位メートルなどが使われるが(正規画像座標系での単位は無次元)，この変換で注目の対象が画像上に移り，一般にそれは離散的なデータであるため\[pixel\]が単位として使われる．

```{=html}
<!-- -->
```
-   $\vec{X_q}$を画像座標系に移す変換は次のように書ける．

$\vec{X_i}=(x_i\ y_i\ 1)^{T}
=\left[
    \begin{matrix} 
        f_x & s &  x_0 \\
        0　 & f_y& y_0 \\
        0  & 0  &  1  \\
    \end{matrix}\right]\vec{X_q}=
    \left[\begin{matrix}
        f_x & s &  x_0 \\
        0　 & f_y& y_0 \\
        0  & 0  &  1  \\
    \end{matrix}\right]
    \left[\begin{matrix}
        x_q \\
        y_q \\
        1   \\
    \end{matrix}\right]$

-   $f_x,f_y$はそれぞれX軸・Y軸方向の焦点距離を表し，単位は\[pixel\]である．
-   これらは，レンズによる拡大縮小の整合性をとるためのスケーリングファクタである．
-   昨今のカメラでは$f_x,f_y$はほぼ同じ値をとる．

ところで，$f_x,f_y$は厳密には焦点距離ではない．そもそも焦点距離の単位は\[m\]というか「長さ」なので，いわゆる無次元である\[pixel\]ではない．
となると$f_x,f_y$の物理的意味は何かというと，「焦点距離」÷「撮像素子幅」に相当する．
画像平面は撮像素子を無限に敷き詰めることができないために離散的であるので，画像は連続な定義域をもたず離散的である．
具体的にどの範囲が一緒くたにされるかというと撮像素子の幅に依存する．
単位の観点では (焦点距離\[m\]) / (撮像素子幅\[m/pixel\])となる．
これを$f_x,f_y$と呼んでいるのである．
焦点距離も撮像素子幅も基本的に定数であるため，$f_x,f_y$もまた定数となる．
当然，レンズが変わればこれらも変わることに注意されたい．

-   $x_0,y_0$は撮像素子の中心の座標であり，同様に単位は\[pixel\]である．
-   これは画像座標系の原点を画像の端(撮像素子の端)にするためのオフセットに相当する．

```{=html}
<!-- -->
```
-   残る$s$はskew coefficient(剪断歪係数)の意味である．
-   その昔技術がまだ乏しいころは撮像素子が平行四辺形だったこともあった．正規画像平面上で正方形であっても画像平面上で平行四辺形になるので，それをモデル化するためのファクタである．
-   しかしそのようなカメラは現在ほぼないので，我々が扱うようなカメラのモデルでは$s=0$になる．

`先の式に現れた，`**`次の行列を(カメラの)内部パラメータと呼び，`$K$`で表す．`**\
$K=\left[
\begin{matrix}
f_x & s & x_0\\
0 & f_y & y_0\\
0 & 0 & 1\\
\end{matrix}
\right]$

### 変換まとめ

-   ワールド座標系の点\$\\vec{X_w}\$は次で表される画像上の点に投影される．

$\vec{X_i}\simeq K[R\ \vec{t}]\ \left[\begin{matrix}\vec{X_W}\\ 1\end{matrix} \right] = 
P\ \left[ \begin{matrix}\vec{X_w}\\1\end{matrix}\right]=
P\tilde{\vec{X_w}}$

-   $\tilde{\vec{X}}$は$\vec{X}$に最下行に1を付与したベクトルである．

`ここで `**$P=K\left[R \ \ \vec{t}\right]$`であり，これを(カメラの)射影行列と呼ぶ.`**\
$P$`はワールド座標にある点をカメラ座標に移すために必要な全てのパラメータを含んでいる．`

-   カメラ行列とか言ったりするきがする．

```{=html}
<!-- -->
```
-   また先の式から相似関係を除去するために，パラメータ$\lambda$を用いて次のように表記することもある．

$\lambda \vec{X_i}=P\tilde{\vec{X_w}}$

### 練習 {#練習_11}

-   ワールド座標のある点が外部パラメータ$[R_1|\vec{t_1}]$,内部パラメータ$K_1$であるようなカメラによって点$\vec{x_i}=\left(x_i,y_i\right)^t$に投影されるとき，
-   外部パラメータが$[R_2|t_2]$,内部パラメータ$K_2$であるようなカメラで撮影したときに，どのような点に投影されうるかを答えよ．
-   なお，レンズ歪は無視して良い．

```{=mediawiki}
{{Collapse_top|たぶんこうなる}}
```
-   定義より

$(x_{i1} y_{i1} 1)^T=\tilde{\vec{X_{i1}}} \sim K_1 [R_1|\vec{t_1}]\tilde{\vec{X_w}}=K_1 [R_1|\vec{t_1}](X_w Y_w Z_w 1)^T$` `

-   スケールファクタ$\lambda$を導入し，

$\lambda K_1^{-1} \tilde{\vec{X_{i1}}} = [R_1|t_1]\tilde{\vec{X_w}}$` `\
$\lambda R_1^{-1}K_1^{-1} \tilde{\vec{X_{i1}}} -R^{-1}\vec{t_1} =\vec{X_w}$` `\
$\lambda R_1^{-1}(K_1^{-1} \tilde{\vec{X_{i1}}}-\vec{t_1}) =\vec{X_w}$

-   これを他方のカメラに投影すればよい．

$\tilde{\vec{X_{i2}}} \sim K_2 [R_2|\vec{t_2}]\tilde{\vec{X_w}}$` `\
$\tilde{\vec{X_{i2}}} \sim K_2 [R_2|\vec{t_2}]\tilde{ \lambda (R_1^{-1}(K_1^{-1} \tilde{\vec{X_{i1}}}-\vec{t_1})})$` `\
$\tilde{\vec{X_{i2}}} \sim K_2 \left(\lambda R_2 R_1^{-1}(K_1^{-1} \tilde{\vec{X_{i1}}}-\vec{t_1}) + \vec{t_2} \right)$` `

-   ところで最後の式は$\tilde{\vec{X_{i2}}}=\lambda \vec{v}+\vec{t}$の形になっている．
-   これが意味するのは，一方で見ている点は他方では直線上のどこかに映るということである．

```{=mediawiki}
{{Collapse_bottom}}
```
### レンズ歪み

-   歪のない像．
    -   全ての水平な線が画像上でも水平になって写っている．

![](1546584960359-無題_1.png "1546584960359-無題_1.png"){width="600"}

-   歪のある像．
    -   外側に行くほど水平な線が湾曲して写っている．

![](1546585188140-Screenshot_from_2019-01-04_15-59-25.png "1546585188140-Screenshot_from_2019-01-04_15-59-25.png"){width="600"}

-   射影行列を定義したのだが，まだモデルに組み込まれていない概念がある．
-   それはレンズの歪みである．
-   この歪みのことを歪曲収差をよぶ．
-   実際のカメラにおいて無視できない歪曲収差には2種類あり，**半径方向歪み**と**円周方向歪み**がある．

```{=html}
<!-- -->
```
-   理想のレンズであれば，並行に入射した光線は全て焦点で交わるはずなのだが，
    現実のレンズでは製造自体が困難であったり，精度の限界という理由で歪が生じる．

#### 半径方向歪み

-   これはレンズの中心から離れた光が，中心付近を通過する光よりも大きく曲げられてしまうことに起因する．

![](1546585530838-upload_f5631edd90867db4d60c38d0f123d6bd.jpg "1546585530838-upload_f5631edd90867db4d60c38d0f123d6bd.jpg"){width="400"}

-   TODO: もっと素敵な画像を用意する

```{=html}
<!-- -->
```
-   上図では，正方形の物体が樽状に投影されている様子である．
-   正方形の辺付近よりも，頂点付近の方がレンズ中心から離れているため，角が丸められた像ができるのである．
-   この歪みを**半径方向歪**と呼ぶ．

```{=html}
<!-- -->
```
-   半径方向歪みはレンズ中心からの距離$r_{q}=\sqrt{x_{q}^2+y_{q}^2}$のテイラー展開でモデル化するのが主流である．
-   撮像素子上にある点の位置は次式で表される．

$\hat{x_{q}}=x_{q}(1+k_1r_q^2+k_2r_q^4+k_3r_q^6)\\
\hat{y_{q}}=y_{q}(1+k_1r_q^2+k_2r_q^4+k_3r_q^6)$

-   見ての通り$k_1,k_2,k_3$が半径方向歪のパラメータである．

#### 円周方向歪み

-   カメラの組み付け誤差により，下図のように撮像素子がレンズと平行でなく組み付けられてしまうと，
-   像が射影変換されてしまい像が正しく映らない．
-   図では，長方形が台形に投影されている様子を示している．
-   この歪を**円周方向歪み**と呼ぶ．

![](1546585587260-upload_a73368045a33cc08bdf1079c2d202051.jpg "1546585587260-upload_a73368045a33cc08bdf1079c2d202051.jpg"){width="400"}

-   TODO: もっと素敵な画像を用意する

```{=html}
<!-- -->
```
-   撮像素子上にある点の位置は次式で表される．

$\hat{x_{q}}=x_{q}+\left(2p_1xy+p_2(r^2_q+2x^2)\right)\\
\hat{y_{q}}=y_{q}+\left(2p_2xy+p_1(r^2_q+2y^2)\right)\\$

-   見てのとおり，$p_1,p_2$が円周方向歪のパラメータである．

#### 歪補正

-   2種類の歪みについて言及した．
-   それらは5種類の歪み係数$k_1,k_2,k_3,p_1,p_2$で特徴づけられた．
-   OpenCVではこれらのパラメータを5次元の列ベクトルで表現することになっている．
    -   また歪みを補正し，歪んでいない像を与える
        cv::undistort関数が実装されている．

### 練習 {#練習_12}

-   適当なカメラ(スマホやノートPCやkinect)などで，RGB画像を撮影し，直線がどのように湾曲するか観察せよ．

## カメラキャリブレーション

-   カメラの内部行列・外部行列・歪み行列を求めることを(カメラの)キャリブレーションと呼ぶ．

### 理論

-   これは最も有名なカメラのキャリブレーション方法であるZhangの手法を紹介する．
-   [元の論文(Zhang
    00)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf)

```{=html}
<!-- -->
```
-   いうて，ここはわかる必要もない気がするので，読まなくていいような気がします．

#### 目標

-   我々は内部行列の($s$は省く)4DoFをキャリブレーションしたい．

$f_x,f_y,c_x,c_y$

-   外部行列の 6DoF．

$x,y,z,pitch,yaw,roll$` `

-   歪みパラメータの5DoF．

$k_1,k_2,k_3,p_1,p_2$

-   併せて$4+6+5=15$個のパラメータを推定する．

#### 方法

-   キャリブレーションには以下のような「チェスボード」と呼ばれる以下のような模様を用いる．

![](1546586146886-upload_b6bf301cd73c8523c0de44570f218b80.png "1546586146886-upload_b6bf301cd73c8523c0de44570f218b80.png"){width="400"}

-   ちなみに[円形のパターンのほうが性能がいいという噂があります](https://qiita.com/gou_koutaki/items/a0b232227d655248f22d)．

```{=html}
<!-- -->
```
-   まず，キャリブレーションボードを様々な(ただし，全ての点を見えるような)配置で対象となるカメラで撮影し，格子点(黒と白の正方形の頂点)を検出する．
-   このとき，サブピクセル単位で格子点を検出しておくとキャリブレーションで大きな誤差が発生しにくい．
    -   OpenCVであればチェスボードの格子点を検出するcv::findChessboardCorners()がある．
    -   先の関数では，サブピクセルオーダーで検出してくれないので，
        cv::findCornerSubPix()などを併用すると良い．

```{=html}
<!-- -->
```
-   明らかに格子点は一つの平面上に存在する．(さもないと困るので，なるだけ平面なボードを用意すること．)
-   一般性を失うことなく，格子点が存在する平面を $z=0$ としてよい．
-   すると，格子点の座標と画像平面上の点との関係は次のようになる．

${\lambda\ \vec{\tilde{X_i}}}=
\left[\begin{matrix}x_i\\y_i\\1\end{matrix}\right]
=K \left[\vec{r_1}\ \vec{r_2}\ \vec{r_3}\ \vec{t} \right]
\left[\begin{matrix}x_w\\y_w\\0\\1\end{matrix}\right]
=K \left[\vec{r_1}\ \vec{r_2}\ \vec{t} \right]
\left[\begin{matrix}x_w\\y_w\\1\end{matrix}\right]$

$\vec{\tilde{X_i}}=H\vec{\tilde{X_w}}$

-   ここで，$\vec{r_1}\sim\vec{r_3}$は回転行列$R$を列ベクトルに分解したもの．
-   $H$は平面オブジェクト上の点を撮像面に投影するホモグラフィ行列であり，
-   $H=\frac{1}{\lambda}K\left[\vec{r_1}\ \vec{r_2}\ \vec{t}\right]$で定義される．
    -   なお$\lambda$はスケールファクタである．

```{=html}
<!-- -->
```
-   3x3行列である$H$は高々9の自由度しか持たないため，対応点の座標組から導ける．

#### 詳細

$\left[
\begin{matrix}
x_w^{(1)}&y_w^{(1)}&1& 0&0&0& 0&0&0 \\
0&0&0& x_w^{(1)}&y_w^{(1)}&1&  0&0&0\\
0&0&0& 0&0&0& x_w^{(1)}&y_w^{(1)}&1 \\
x_w^{(2)}&y_w^{(2)}&1& 0&0&0& 0&0&0 \\
0&0&0& x_w^{(2)}&y_w^{(2)}&1&  0&0&0\\
0&0&0& 0&0&0& x_w^{(2)}&y_w^{(2)}&1 \\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
x_w^{(i)}&y_w^{(i)}&1& 0&0&0& 0&0&0 \\
0&0&0& x_w^{(i)}&y_w^{(i)}&1&  0&0&0\\
0&0&0& 0&0&0& x_w^{(i)}&y_w^{(i)}&1 \\
\end{matrix}
\right]
\left[
\begin{matrix}
H_{11}\\H_{12}\\H_{13}\\
H_{21}\\H_{21}\\H_{23}\\
H_{31}\\H_{31}\\H_{33}
\end{matrix}
\right]=
\left[
\begin{matrix}
x_i^{(1)}\\y_i^{(1)}\\1\\
x_i^{(2)}\\y_i^{(2)}\\1\\
\vdots\\
x_i^{(i)}\\y_i^{(i)}\\1\\
\end{matrix}
\right]$

-   ここで，$x_w^{(i)},x_i^{(i)}$は同じキャリブレーションボードの配置のもとで検出された$i$番目の点でのワールド座標系でのx値，画像座標系でのx値を指す．
-   ところでホモグラフィー行列$H$は外部パラメータを含むので，キャリブレーションボードの配置が異なる画像からは，異なる$H$が得られる．

```{=mediawiki}
{{Collapse_top |詳細}}
```
-   次に，ホモグラフィ行列$H$を次のように列ベクトルで分解する．

$H=\left[\vec{h_1}\ \vec{h_2}\ \vec{h_3}\right]$

また，$H$の定義より，
$\left[\vec{h_1}\ \vec{h_2}\ \vec{h_3}\right]=\frac{1}{\lambda}
K\left[\vec{r_1}\ \vec{r_2}\  \vec{t}\right]$ が成立する．

-   $\vec{r_1},\vec{r_2}$が直交することを踏まえると次のような変形ができる．

$\vec{r_1}^T\vec{r_2} =0 \\
\vec{h_1}^T K^{-T}K^{-1}\vec{h_2} =0$

-   また回転行列$R$は直交行列であることから，各行ベクトルの大きさは等しい．
-   故に，

$||\vec{r_1}|| =||\vec{r_2}|| \\
\vec{r_1}^T\vec{r_1} = \vec{r_2}^T\vec{r_2} \\
\vec{h_1}^TK^{-T}K^{-1}\vec{h_1} = \vec{h_2}^TK^{-T}K^{-1}\vec{h_2}$

-   ここで新たに行列$B$を次のように定義する．

$B=K^{-T}K^{-1}$

-   この行列は明らかに次のような要素を持つ．

$B=\left[
\begin{matrix}
\frac{1}{f_x^2} & 0 & \frac{-c_x}{f_x^2}\\
0 & \frac{1}{f_y^2}  & \frac{-c_y}{f_y^2}\\
\frac{-c_x}{f_x^2} &  \frac{-c_y}{f_y^2} & \frac{c_x^2}{f_x^2}+\frac{c_y^2}{f_y^2}+1\\
\end{matrix}
\right]$

-   今，我々は2つの拘束条件をもっている．

$\vec{h_1}^TB\vec{h_2}=0\\
\vec{h_1}^TB\vec{h_1}=\vec{h_2}^TB\vec{h_2}\\$

-   $B$は明らかに対称行列であることに注意すると，上の式の左辺は次のように展開できる．

$\vec{h_i}^TB\vec{h_j} = \vec{v_{ij}}^T\vec{b} =
\left[\begin{matrix}
H_{i1}H_{j1}\\
H_{i1}H_{j2}+H_{i2}H_{j1}\\
H_{i2}H_{j2}\\
H_{i3}H_{j1}+H_{i1}H_{j3}\\
H_{i3}H_{j2}+H_{i2}H_{j3}\\
H_{i3}H_{j3}\\
\end{matrix}\right]^T
\left[\begin{matrix}
B_{11}\\
B_{12}\\
B_{22}\\
B_{13}\\
B_{23}\\
B_{33}\\
\end{matrix}\right] \hspace{10mm} i,j\in1,2$

-   ただし
    $\vec{b}=\left[B_{11}\ B_{12}\ B_{22}\ B_{13}\ B_{23}\ B_{33}\right]^T$
-   また `<math>`{=html}

\\vec{v\_{ij}}=\\left\[ H\_{i1}H\_{j1}\\ \\
H\_{i1}H\_{j2}+H\_{i2}H\_{j1}\\ \\ H\_{i2}H\_{j2}\\ \\
H\_{i3}H\_{j1}+H\_{i1}H\_{j3}\\ \\ H\_{i3}H\_{j2}+H\_{i2}H\_{j3}\\ \\
H\_{i3}H\_{j3} \\right\]\^T`</math>`{=html}

-   $\vec{v_{ij}}$を先のように定義すると，拘束条件は次のようにまとめられる．

$\left[\begin{matrix}
\vec{v_{12}}^T \\
(\vec{v_{11}}-\vec{v_{22}})^T 
\end{matrix}\right]\vec{b}=\vec{0}$

-   ところで，上の2x6の係数行列を持つ連立方程式の係数はホモグラフィ行列$H$の要素で構成されている．
-   既に$H$の求め方は示してあるので，我々は$\vec{b}$を求めるのに十分な情報を持っている．

```{=html}
<!-- -->
```
-   また，先の方程式はキャリブレーションボードの画像毎に得ることができるので，
-   もし $k$
    枚の画像を用意できるのであれば，この方程式を$k$個縦に並べることができる．

$\left[\begin{matrix}
\vec{v_{12}^{<1>}}^T \\
(\vec{v_{11}^{<1>}}-\vec{v_{22}^{<1>}})^T \\
\vec{v_{12}^{<2>}}^T \\
(\vec{v_{11}^{<2>}}-\vec{v_{22}^{<2>}})^T \\
\vdots\\
\vec{v_{12}^{<k>}}^T \\
(\vec{v_{11}^{<k>}}-\vec{v_{22}^{<k>}})^T \\
\end{matrix}\right]\vec{b}=\vec{0}$

-   これは2x6の係数行列を持ち，未知数が6個であるから解くことができる．
-   冗長に見えるかもしれないが格子点の座標はノイズが乗りがちなので，
-   精度を高めるためには多くの情報を使うにこしたことはない．

```{=html}
<!-- -->
```
-   $B$によって内部パラメータは次のように表せられる．

$\lambda = B_{33}-\frac{B_{13}^2+c_y(B_{12}B_{13}-B_{11}B_{23})}{B_{11}}\\
f_x = \sqrt{\frac{\lambda}{B_{11}}}\\
f_x = \sqrt{\frac{\lambda}{B_{11}}}\\
c_x = -\frac{B_{13}f_x^2}{\lambda}\\
c_y = \frac{B_{12}B_{13}-B_{11}B_{23}}{B_{11}B_{22}-B_{12}^2}\\$

-   $\lambda$は正規直交の条件式$\lambda = \frac{1}{||K^{-1}\vec{h_{1}}||}$からきている．
    -   $\vec{r_1}$は正規直交行列の行ベクトルなので，そのノルムは1になるはずである．
    -   $\vec{r_1}=\lambda K^{-1}\vec{h_1}$から先の条件が得られる．

```{=html}
<!-- -->
```
-   同様に外部パラメータは次のように表すことができる．

$\vec{r_1} = \lambda K^{-1}\vec{h_1} \\
\vec{r_2} = \lambda K^{-1}\vec{h_2} \\
\vec{r_3} = \vec{r_1}\times\vec{r_2} \\
\vec{t}   = \lambda K^{-1}\vec{h_3}$

-   ところが，上の式で得られた$\vec{r_1},\vec{r_2},\vec{r_3}$を並べただけの行列
    $R=[\vec{r_1} \vec{r_2} \vec{r_3}]$
    は回転行列の性質$RR^{T}=R^{T}R=I$を満たさないことが多い．
-   というわけで，得られた$R$になるだけ近い回転行列を求めたい．
-   そのために特異値分解を使う．
    -   詳しい話はしないが，特異値分解はフロベニウスノルムの差が最小であるような行列を生成することができる．

```{=html}
<!-- -->
```
-   具体的には，$R=U\Sigma V^T$のように分解したあと，$\Sigma$を$I$で置換するだけでよい．
-   つまり，$\tilde{R}=UIV^{T}$こそが，$R$に最も近い回転行列である．

```{=html}
<!-- -->
```
-   以上で，外部パラメータ$R,\vec{t}$と内部パラメータ$K$を求めることができたので，残りの歪みパラメータを求める．
-   といっても，すでに他のパラメータが得られていることから，次の方程式を解くだけで済む．

$\left[\begin{matrix} x_{i}\\ y_{i}\end{matrix}\right]=
(1+k_1r^2+k_2r^4+k_3r^4)
\left[\begin{matrix}x_{d} \\ y_{d}\end{matrix}\right]+
\left[\begin{matrix}
    2p_1x_dy_d+p_2(r^2+2x_d^2) \\ 
    2p_2x_dy_d+p_1(r^2+2y_d^2)
\end{matrix}\right]$

-   ここで，$x_d,y_d$は歪みよって移された像の座標であり，$x_i,y_i$は歪が全く無かった場合の座標である．

```{=mediawiki}
{{Collapse_bottom}}
```
### 校正方法

-   OpenCVのサンプルにもあるのですが，[KYabuuchi/camera-calibration](https://git.robotech.t.u-tokyo.ac.jp/KYabuuchi/camera-calibration)を作りました．
    -   とりあえず実行すれば，サンプルファイルを読み込んでキャリブレーションする雰囲気を楽しめます．
    -   OpenCVのサンプルよりも手軽に雰囲気を楽しめます．

### 注意

-   内部パラメータKは焦点距離や撮像素子の座標などが含まれるので，カメラのレンズを交換したりしたときは，改めてキャリブレーションする必要がある．
-   外部パラメータは空間座標でのカメラの位置を移動されば当然変るので，改めてキャリブレーションする必要がある．
-   「Zhangの手法を使うときに必要な画像枚数は」これってトリビアになりませんか?
    -   撮影枚数がk枚，チェスボード上の格子点数をnとすると
        2nk\>=6k+4が必要十分条件
-   ~~kinect-v2はキャリブレーションしなくてもいい　みたいな記述をwiki内で見た気がする~~
    -   そんなことはなかった．
    -   以下はkinect-v2で取得したRGBとDepth．上が生で，下が除歪したもの．
    -   Depthは画面左の柱が湾曲している様が見て取れる

![](1553871227638-rgb.png "1553871227638-rgb.png"){width="400"}
![](1553871227652-depth.png "1553871227652-depth.png"){width="300"}

### 練習 {#練習_13}

-   適当なカメラでキャリブレーションし，校正したものとそうでないもので画像を比較せよ．
    -   具体的にチェスボードを印刷などして，様々な姿勢でそれを撮影し，その画像から内部パラメータ(4個)と歪みパラメータ(5個)を推定せよ．
    -   推定及び校正するプログラムはOpenCVのサンプルを用いても良いし，自作しても良いし，[KYabuuchi/camera-calibration](https://git.robotech.t.u-tokyo.ac.jp/KYabuuchi/camera-calibration)を使っても良い．

## 局所特徴量

-   画像感の対応点を求めるときに，画像内の特徴点を検出して，その特徴点の対応を探索することがあります．
-   というより古典CVはまず初手で特徴点をとります．
    -   DNNを用いるなら，特徴点の検出から分類・識別までひとっ飛びできるので必要ないのです．
-   さて特徴点としてどういうものを取るとロバストに処理できるでしょうか．
-   OpenCVチュートリアルでは[2D Features framework (feature2d
    module)](https://docs.opencv.org/3.2.0/d9/d97/tutorial_table_of_content_features2d.html)に相当します．

### 検出

-   例えば以下の左の画像から右の画像に対応する箇所を探す場合を考えましょう．
    -   このような問題をtemplate matchingと呼びます

![](1546753134947-局所特徴量.png "1546753134947-局所特徴量.png"){width="600"}

-   人間であれば頭部付近に対応していることがなんとなくわかります．
-   コンピュータでこれを探そうと思ったら，左の画像に右の画像を重ねて類似度を計算し，少しずらして類似度を計算しを繰り返して最も類似度の高い箇所を探さないといけません．
    -   詳しい話は，ラスタースキャンやSSD(Sum of Squared
        Difference)やSAD(Sum of Absolute
        Difference)などで調べてください．
-   さて次の場合はどうでしょう．

![](1546753283705-局所特徴量2.png "1546753283705-局所特徴量2.png"){width="600"}

-   このような変化があっても，人間であれば頭部付近に対応していることがなんとなくわかります．
-   しかし計算機で行う場合は，色の変化以外は先の類似度を走査する手法は使えません．
-   というわけで，方針を変えて，要求画像を画像として扱うことを辞めて，特徴的な点を含む画像の集合とみなします．
-   要求画像と元画像の双方で特徴的な点を検出し，それらの対応関係(どの点がどの点に一致するか)を求めることで，画像の対応関係を導きます．
-   このとき特徴点には次の条件がもとめられます．
    -   **再現性**: 同じ画像であれば同じ場所から特徴点を検出できる．
    -   **スケール不変性**:
        スケールが異なる画像でも対応する同じ場所から特徴点を検出できる．
    -   **明度不変性**:
        画像の明るさが変わっても同じ場所から特徴点を検出できる．
    -   **回転不変性**: 画像が回転しても同じ場所から特徴点を検出できる．
-   厳しい条件のようですが，これらを満たす特徴量検出の手法が多数開発されています．
-   ここではその実現方法は解説しませんが，[画像処理の数式を見て石になった時のための、金の針](https://qiita.com/icoxfog417/items/adbbf445d357c924b8fc)にSIFT(Scale
    Invariant Feature Transform)特徴の解説があります．
-   局所特徴量は原則，色の違いを見ません．
    -   それにより色相が変化したり，全体的に明るくなっても対応できるわけですが，
    -   色の雰囲気が似てる点だからといって同様に検出されるわけではありません．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    cv::Mat image = cv::imread("image.jpg");
    cv::Ptr<cv::FeatureDetector> detector = cv::AKAZE::create();
    std::vector<cv::KeyPoint> keypoints;
    detector->detect(image, keypoints);
    cv::drawKeypoints(image, keypoints, image, cv::Scalar::all(-1), cv::DrawMatchesFlags::DRAW_RICH_KEYPOINTS);
    cv::imshow("AKAZE Keypoints", image);
    cv::waitKey(0);
    return 0;
}
```

![](1548588332666-Screenshot_from_2019-01-27_20-25-23.png "1548588332666-Screenshot_from_2019-01-27_20-25-23.png"){width="400"}

-   検出した(AKAZE)特徴点に印をつけた図です．
-   印の色はランダムで，意味はありません．

### 記述

-   さて，特徴点を検出できると仮定してもまだ課題があります．
-   1枚の画像から特徴点が多数検出できるわけですが，どの特徴点同士が対応関係にあるのかがわかる必要があります．
-   そのために，任意の特徴点にその点付近で画像がどのような姿をしているかの情報をもたせる必要があります．
-   さらにその情報もまた，スケール不変性・回転不変性・明度不変性を持つ必要があります．
-   さらに，似たような特注点であれば近い特徴量を持つ必要があります．
    -   例えば，幾つかの特徴量はn次元ベクトルで書かれます．
    -   n次元空間での近さが画像上での特徴点の見た目?の近さに対応しなければいけません．
    -   また画像上で遠い特徴点は特徴量空間でも遠い必要があります．
-   この情報のことを局所特徴量と呼びます．
-   これまた大変そうですが，普遍な特徴点を検出できたのだからできそうですよね．
-   ここではその実現方法は解説しませんが，[画像処理の数式を見て石になった時のための、金の針](https://qiita.com/icoxfog417/items/adbbf445d357c924b8fc)にSIFT(Scale
    Invariant Feature Transform)特徴の解説があります．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <iostream>
#include <opencv2/opencv.hpp>
int main()
{
    cv::Mat image = cv::imread("image.jpg");
    cv::Ptr<cv::FeatureDetector> detector = cv::AKAZE::create();
    std::vector<cv::KeyPoint> keypoints;
    cv::Mat descriptors;
    detector->detect(image, keypoints);
    detector->compute(image, keypoints, descriptors);
    // 特徴点の数
    std::cout << keypoints.size() << std::endl;
    // 特徴量ベクトルの次元x特徴点の数(次元は記述子の種類に依る)
    std::cout << descriptors.size() << std::endl;
    // 1個目の特徴点の座標
    std::cout << keypoints.at(0).pt << std::endl;
    // 1個目の特徴量ベクトルの成分
    std::cout << descriptors.row(0) << std::endl;
    return 0;
}
```

-   ここまでで，普遍(不変?)な特徴点の検出と，普遍な記述ができたので，あとは似たような特徴点を探すだけで目標が達成されます．
-   特徴量は特徴量空間での距離と，画像上でのみための近さに相関があるので，特徴量のL2ノルムとかで似た特徴点を探せます．
    -   AKAZEやORBであればハミング距離を利用できて，マッチングが速いです．
-   マッチングは一般にミスマッチを含むため，何かしらの工夫でミスマッチを除去する必要があります．
    -   例えば，A-\>Bで候補を選出して，B-\>Aでも候補を選出して，互いに最も近いと判定したものだけを採用する方法があります．
-   以下には，1番近い候補と2番目に近い候補の一致度(距離)がある程度離れていることを条件として，優れたマッチングを生成しています．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <iostream>
#include <opencv2/opencv.hpp>
int main()
{
    cv::Mat image1 = cv::imread("image1.jpg");
    cv::Mat image2 = cv::imread("image2.jpg");
    cv::Ptr<cv::FeatureDetector> detector = cv::AKAZE::create();
    std::vector<cv::KeyPoint> keypoints1, keypoints2;
    cv::Mat descriptors1, descriptors2;
    detector->detectAndCompute(image1, cv::noArray(), keypoints1, descriptors1);
    detector->detectAndCompute(image2, cv::noArray(), keypoints2, descriptors2);
    cv::BFMatcher matcher(cv::NORM_HAMMING);
    std::vector<std::vector<cv::DMatch>> knn_matches;

    // descriptors1の各ベクトルについて近いベクトルを2個ずつdescriptors2から選ぶ
    matcher.knnMatch(descriptors1, descriptors2, knn_matches, 2);

    // 2番近い候補の距離の0.7倍よりも1番目に近い候補の距離が近いなら，良い候補
    const float ratio_thresh = 0.7f;
    std::vector<cv::DMatch> good_matches;
    for (size_t i = 0; i < knn_matches.size(); i++) {
        if (knn_matches[i][0].distance < ratio_thresh * knn_matches[i][1].distance) {
            good_matches.push_back(knn_matches[i][0]);
        }
    }
    cv::Mat show_image;
    cv::drawMatches(image1, keypoints1, image2, keypoints2, good_matches, show_image, cv::Scalar::all(-1), cv::Scalar::all(-1), std::vector<char>(), cv::DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS);
    cv::imshow("window", show_image);
    cv::waitKey(0);
    return 0;
}
```

![](1548593025457-Screenshot_from_2019-01-27_21-39-23.png "1548593025457-Screenshot_from_2019-01-27_21-39-23.png"){width="700"}

-   マッチングを行った図です．
-   1つの画像から一部を切り出して，回転したものを2枚目の画像として採用しました．
-   まったく関係ない3点ともマッチングしてしまっています．
-   回転しているため，わかりにくいですが，だいたいあっているような気がします．
-   [CV：初音ミク](https://wiki.famitsu.com/kairi/%E3%82%AD%E3%83%A3%E3%83%A9%E3%82%AF%E3%82%BF%E3%83%BC%E4%B8%80%E8%A6%A7/%E3%80%90%E5%A5%8F%E3%81%A7%E3%82%8B%E4%BA%94%E7%B7%9A%E3%80%91%E7%95%B0%E7%95%8C%E5%9E%8B%E9%9B%AA%E3%83%9F%E3%82%AF2014)らしいです．CV：初音ミク(CV：藤田咲)のが厳密な気がします．

### 種類

-   OpenCV内に，特徴点検出と特徴量記述はfeatures2d(core)とxfeatures2d(contrib)内にあるので，どちらを使うかで微妙に宣言の仕方が違います．
-   [このページが](https://qiita.com/hmichu/items/f5f1c778a155c7c414fd)まとまっています．

  -------------------------- ---------------------------- --------------
  名前                       手法                         特徴量の表現
  cv::GFTTDetector           goodFeaturesToTrack (検出)   \-
  cv::AgastFeatureDetector   AGAST (検出)                 \-
  cv::FastFeatureDetector    FAST (検出)                  \-
  cv::MSER                   MSER(検出)                   \-
  cv::BRISK                  BRISK                        バイナリ
  cv::KAZE                   KAZE                         スケール
  cv::ORB                    ORB                          バイナリ
  cv::AKAZE                  A-KAZE                       バイナリ
                                                          
  -------------------------- ---------------------------- --------------

  ------------------------------------------- --------------------- --------------
  名前                                        手法                  特徴量の表現
  cv::xfeatures2d::StarDetector               StarDetector (記述)   \-
  cv::xfeatures2d::MSDDetector                MSD (検出)            \-
  cv::xfeatures2d::LATCH                      LATCH(記述)           バイナリ
  cv::xfeatures2d::LUCID                      LUCID(記述)           ?
  cv::xfeatures2d::BriefDescriptorExtractor   BRIEF (記述)          バイナリ
  cv::xfeatures2d::DAISY                      DAISY (記述)          実数ベクトル
  cv::xfeatures2d::FREAK                      FREAK (記述)          バイナリ
  cv::xfeatures2d::SIFT                       SIFT                  実数ベクトル
  cv::xfeatures2d::SURF                       SURF                  実数ベクトル
  ------------------------------------------- --------------------- --------------

### サブピクセル

-   サブピクセルとはピクセル(整数値)よりも細かい単位の画素座標であり，つまるところ，少数第一位以降にも有効数字の並ぶ座標系です．
-   ディジタル画像は離散的な座標にしか画素情報を持たないので，画素の間とかに極値をもったりすると，その後の計算で計算誤差を引きずってしまいます．
-   これまでの例では特徴点は実はサブピクセル単位で点の座標が得られています．(cv::KeyPointのptを見ると少数が入っている)
-   ただしOpenCVの一部の関数(cv::goodFeaturesToTrackやcv::cornerHarrisなど)ではピクセル単位でしか座標を計算してくれません．
-   そういうときはcv::cornerSubPix
    を用いるとサブピクセル単位で特徴点を検出できます．
-   [これ](https://docs.opencv.org/3.2.0/d8/d5e/tutorial_corner_subpixeles.html)が例．

### 練習 {#練習_14}

-   少なくとも2種類以上の局所特徴量で以下の画像から特徴点を取り出し，それぞれ検出数と計算時間を比較せよ．
    -   また，検出できた点の分布について考察せよ．

![](1546464969904-DSC_0426.JPG "1546464969904-DSC_0426.JPG"){width="600"}
![](1546474864735-DSC_1045.JPG "1546474864735-DSC_1045.JPG"){width="600"}

### サンプル

-   この後の章では局所特徴量を取り出して，マッチングをとるという操作が複数回行われるので関数を定義しておきます．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
std::pair<std::vector<cv::Point2f>, std::vector<cv::Point2f>> getMatching(const cv::Mat& image1, const cv::Mat& image2, const float threshold = 0.7f)
{
    cv::Ptr<cv::FeatureDetector> detector = cv::AKAZE::create();
    std::vector<cv::KeyPoint> keypoints1, keypoints2;
    cv::Mat descriptors1, descriptors2;
    std::vector<std::vector<cv::DMatch>> knn_matches;
    cv::BFMatcher matcher(cv::NORM_HAMMING);

    // 検出->記述->マッチング
    detector->detectAndCompute(image1, cv::noArray(), keypoints1, descriptors1);
    detector->detectAndCompute(image2, cv::noArray(), keypoints2, descriptors2);
    matcher.knnMatch(descriptors1, descriptors2, knn_matches, 2);

    // 優れたマッチングのみ取り出す
    std::vector<cv::KeyPoint> good1, good2;
    for (size_t i = 0; i < knn_matches.size(); i++) {
        if (knn_matches[i][0].distance < threshold * knn_matches[i][1].distance) {
            good1.push_back(keypoints1.at(knn_matches[i][0].queryIdx));
            good2.push_back(keypoints2.at(knn_matches[i][0].trainIdx));
        }
    }

    // cv::Point2fのが都合がいいことが多い
    std::vector<cv::Point2f> points1, points2;
    cv::KeyPoint::convert(good1, points1, std::vector<int>());
    cv::KeyPoint::convert(good2, points2, std::vector<int>());

    return {points1, points2};
}
```

## Epipolar Geometry {#epipolar_geometry}

-   Epipolar
    Geometryとは複数の視点から同一の対象物を見たときの幾何学的な制約を取り扱う分野です．
-   カメラが複数台ある場合や，カメラが時間とともに移動する場合の３次元構造と画像への投影のされかたについて考えます．

![](1535623733471-DRQONGsV4AAXIkV.jpg "1535623733471-DRQONGsV4AAXIkV.jpg"){width="400"}これは違う．

### Epipolar Plane {#epipolar_plane}

![](1561308339673-Screenshot_from_2019-06-24_01-45-20.png "1561308339673-Screenshot_from_2019-06-24_01-45-20.png"){width="800"}

-   最初に図の見方と各要素の名称を定義していきます．
-   上の図は$x_w$という3次元空間の点を2個のカメラを用いて撮影している様子です．
    -   薄暗い平行四辺形の枠はそれぞれのカメラ座標系でz=1の正規化平面を表しています．
    -   これは画像平面ではないので，いわゆるピクセルとは関係ありません．とうぜん焦点距離などを含む内部パラメータはこの図では関係ありません．
        -   (ところで，$\vec{x_c}$のノーテーションで書いてますが，正規化座標系なので$\vec{x_q}$で書くのが定義として素直な気がします．ただ正規化座標系を陽に使わない文献などが存在するので，ここではカメラ座標系としての添字を付与しています．)
-   カメラはそれぞれ$\vec{O_{c1}},\vec{O_{c2}}$にあります．
-   それら3点を通る平面のことを**Epipolar Plane**と呼びます．
-   それぞれのカメラの正規化平面には空間の点が$\vec{x_{c1}},\vec{x_{c2}}$に投影されています．
-   カメラ1の画像平面上には空間の点の他にカメラ2の原点も投影されています．
    -   この投影点のことを**Epipole**と呼びます．図では$\vec{e_{c1}}$．
-   同様にカメラ2の画像平面上にもカメラ1の原点が投影されています．

```{=html}
<!-- -->
```
-   先の練習問題でも扱ったように，一方のカメラに投影された1点というのは他方のカメラでは直線で写ります．
-   具体的には，$\vec{O_{c1}},\vec{x_w}$を結ぶ直線がカメラ1の画像平面上では1点($\vec{x_{c1}}$)に投影されていますが，カメラ2の画像平面上には直線$l_2$として投影されています．
-   カメラ2の画像座標に依って定まるカメラ1の画像座標上の直線のことを**Epipolar
    Line**と呼びます
    -   直線と点の色の対応に注意して読み取ってください．
-   $l_1$も同様です．

```{=html}
<!-- -->
```
-   これで定義はひとまず終わりです．
-   次はこれらの性質について見ていきます．

`Epipolar lineは必ずEpipoleを通る`

∵Epipolar
Lineは他方のカメラの対象点を観測する光軸を自身の画像平面上に投影したものなので，当然他方のカメラを投影したEpipoleを通過する．

`カメラが平行であるとき，Epipoleは無限遠に生じる`

∵図のカメラがどんどん並行になっていく様子を想像すると理解しやすいかもしれない．特に並行であるとき，すべてのEpipolar
Lineは並行になるという性質がある．

`Epipolar Line`$l_1$`はカメラの相対姿勢`$R_{12},\vec{t_{12}}$`と他方の正規化平面上での座標`$\vec{x_{c2}}$`によって定まる`

∵むしろ他に介入できそうなパラメータがない．あえていうなら内部パラメータがありそうだが，この図には関係ない．

-   性質を見ることで，それぞれがどういう概念かの感覚をつかめたと思います．
-   次の項では，具体的に$l_1$がどのように計算されるかを見ていきます．

### Essential Matrix & Fundamental Matrix & Epipolar Equation {#essential_matrix_fundamental_matrix_epipolar_equation}

-   Epipolar Lineが満たす性質について考えていきます．
-   先の項の図を見ながら読み進めてください．ちょっと複雑です．
-   Epipolar Planeの法線ベクトル$\vec{l}$は以下の性質を満たします．

$\vec{l}\simeq \vec{e_{c1}} \times \vec{x_{c1}} \simeq \vec{t} \times ( R\lambda\vec{x_{c2}} +\vec{t}) = [\vec{t}]_{\times} R \lambda\vec{x_{c2}}  \simeq [\vec{t}]_{\times} R \vec{x_{c2}}$

-   \'ℓ\'の書体が違うのは無視してください．
-   一個づつ見ていきましょう．
    -   最初の相似式はいいですね．面の法線ベクトルはその面を張る独立なベクトルの外積に並行です．
    -   2個目の相似式は２つのことを同時にやっています．まず$\vec{t}/t_z=\vec{e_{c1}}$を利用しています．
        -   そして$\vec{x_{c1}} \simeq R(\lambda \vec{x_{c2}})+\vec{t}$を適用しています．これはカメラ座標系の距離1である正規化平面上の点を深度$\lambda$を掛けて遠くに飛ばし，カメラ間の相対姿勢を適用することで，カメラ1で見たときの座標に座標変換しているのです．
    -   3個目はまず，$\vec{t}\times \vec{t}=\vec{0}$を抑えましょう．あとは次の表現を採用しただけです．

$[\vec{t}]_{\times}= \left[ \begin{matrix} 0&-t_z&t_y\\t_z&0&-t_x\\-t_y&t_x&0\\ \end{matrix}\right]$

-   つまるところ，ベクトル×(行列・ベクトル)ってベクトルになるじゃないですか，行列・行列・ベクトルもベクトルになるじゃないですか．あとは外積がキモいので行列に置き換えたと思ってもらうといいと思います．
-   結局$\vec{l}\simeq[\vec{t}]_{\times}R\vec{x_{c2}}$という式が得られました．

```{=html}
<!-- -->
```
-   さらに変形を続けます．

$\vec{0} = \vec{x_{c1}}^T \vec{l} = \vec{x_{c1}}^T [\vec{t}]_{\times}R \vec{x_{c2}}$

-   最初の等式は明らかだと思います．直交するベクトルの内積は0になりますね．
    後段は先の式を代入しただけです．
-   というわけで$\vec{x_{c1}},\vec{x_{c2}}$を結びつける式が得られました．
-   とっても有用です．有用すぎて名前がついています．

`次の式を`**`Epipolar Equation`**`と呼びます．`\
$\vec{x_{c1}}^T [\vec{t}]_{\times}R \vec{x_{c2}} = \vec{0}$\
`また，次で定義されるEを`**`Essential Matrix`**`と呼びます．(日本語だと「基礎行列」だとか「E行列」と呼ばれます．)`\
$E= [\vec{t}]_{\times}R$

-   E行列はその定義から，カメラ間の相対姿勢のみで定義されます．
-   これがどう有用かというと，片方のカメラで観測された点が他方のカメラで映るときの場所(のヒント)を教えてくれるのです．
-   具体的には，例えば，カメラ2で$\vec{x_{c2}}$に対象物を観測したとしましょう．
-   そのとき，カメラ1で観測される場所$\vec{x_{c1}}$はその方程式を満たすわけです．
-   ところで，先に式を少し書き換えてみます．

$\vec{x_{c1}} \cdot  \vec{l_1} = \vec{0}$

-   既知のところで全部計算して$\vec{l_1}$で置き換えてみました．
-   これは「空間の直線の式」です．
-   察していただけると思うのですが，これは上の図におけるEpipolar
    Lineを与える方程式ですね．

```{=html}
<!-- -->
```
-   すこしまとめましょう．

`相対姿勢がわかる ．⇨ E行列が計算できる． ( `$\vec{x_{c1}}^T E \vec{x_{c2}} = \vec{0}$`)`\
`さらに，片方のカメラで観測したときの座標がわかる．⇨Epipolar Lineが計算できる．( `$\vec{x_{c1}} \cdot  \vec{l_1} = \vec{0}$`)`\
`対象点をカメラ1で探したければ，対象のEpipolar Line上のみを調べればよい．`\
`せっかくなので少し休憩しましょう．`[`never ender`](https://www.youtube.com/watch?v=z5A0joDLq38&list=PLY4pDevDNKpNDH3Pp9fwYJ3YiKIUdb6jL&index=6&t=0s)`という曲はいかがでしょうか?`

-   カメラ2個ではこれ以上は計算できません．(∵かなり上で解いた練習問題より)

```{=html}
<!-- -->
```
-   ここまでの話は，カメラ座標系$\vec{x_{c}}$(正規座標系$\vec{x_{q}}$)で行われてきましたが，可能であれば画像座標系でことを済ませたいです．
-   というのも，多くの場合我々が画素単位のデータを取得するので．
-   というわけで，先のEpipolar方程式を画像座標系$\vec{x_i}$を用いて表してみます．

$\tilde{\vec{x_i1}} \simeq K_{1} \vec{x_c1} \\ \tilde{\vec{x_i2}} \simeq K_{2} \vec{x_c2}$

-   を踏まえて，

$\vec{x_{c1}}^T E\vec{x_{c2}} = \vec{0}\\
\vec{x_{i1}}^T K_{1}^{-T}E K_2^{-1}\vec{x_{i2}} = \vec{0}$

-   とかけます．

`* 以下の式も`**`Epipolar Equation`**`呼ばれます．`\
$\vec{x_{i1}}^T F \vec{x_{i2}} = \vec{0} \\
 F= K_{1}^{-T}E K_2^{-1}$\
`* このFは`**`Fundamental Matrix`**`と呼ばれます．日本語だとF行列だとか基礎行列と呼ばれたりします．`

-   まとめると，外部パラメータ$R,\vec{t}$と内部パラメータ$K$が定まれば画像座標系でのEpipolar方程式が建てられるということになります．
-   名前がややこしいので，注意をしましょう．
    -   $E=[\vec{t}]_{\times}R$ : Essential Matrix 基礎行列
    -   $F=K_1^{-T}[\vec{t}]_{\times}RK_2^{-1}$ Fundamental Matrix
        基本行列

#### サンプル {#サンプル_1}

-   既知の姿勢からEpipolar方程式を立てて，検出した特徴点に対するEpipolar
    Lineを描画するサンプルを作ります．
-   画像は以下を利用します．

![](1561457265891-left1.png "1561457265891-left1.png"){width="400"}![](1561457265897-right1.png "1561457265897-right1.png"){width="400"}

-   カメラ間の相対位置は以下のとおりです．

$\vec{t}=(-0.3,0.05,0)^T,R=x軸回りに18deg$\
$K=\left(\begin{matrix} 
500&0&320\\
0&500&240\\
0&0&1
\end{matrix}\right)$\
$D=\left(-0.15,0.83,-2.7e-4,-1.2e-3,-2.3\right)^T$\
![`1561457356425-DSC_1437.jpg`](1561457356425-DSC_1437.jpg "1561457356425-DSC_1437.jpg"){width="200"}

```{=mediawiki}
{{Collapse_top|サンプルコード}}
```
```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    cv::Mat image1 = cv::imread("left1.png"), image2 = cv::imread("right1.png");
    const int W = image1.size().width;
    const double THETA = 3.1415 / 9;  // [rad]=18[deg]
    const cv::Mat R = (cv::Mat1d(3, 3) << 
        1, 0, 0, 
        0, std::cos(THETA), -std::sin(THETA), 
        0, std::sin(THETA), std::cos(THETA));
    const cv::Mat T=(cv::Mat1d(3, 3) << 
        0, 0, 0.05, 
        0, 0, -0.3, 
        -0.05, 0.3, 0); // [m]
    const cv::Mat K = (cv::Mat1d(3, 3) << 
        500, 0,320, 
        0, 500, 240, 
        0, 0, 1);
    const cv::Mat E = T * R;
    const cv::Mat F = K.inv().t() * E * K.inv();
    std::cout << "Essential Matrix\n" << E << "\nFundamental Matrix\n" << F << std::endl;

    cv::Ptr<cv::FeatureDetector> detector = cv::AKAZE::create();
    std::vector<cv::KeyPoint> keypoints; detector->detect(image1, keypoints);

    int i = 0;
    for (const cv::KeyPoint& kp : keypoints) {
        if ((i++) % 30 != 0)
            continue;
        cv::Point3d x_i(kp.pt.x, kp.pt.y, 1);
        cv::Point3d l(cv::Mat(cv::Mat(x_i).t() * F));  // Epipolar Line
        cv::Point2d b1(0, -l.z / l.y), b2(W, -(l.z + l.x * W) / l.y);
        cv::Scalar color = cv::Scalar(rand() % 255, rand() % 255, rand() % 255);
        cv::circle(image1, kp.pt, 5, color, -1); cv::line(image2, b1, b2, color, 2);
    }
    cv::Mat image; cv::hconcat(image1, image2, image);
    cv::imshow("Epipolar Geometry", image);
    cv::waitKey(0);
    return 0;
}
```

```{=mediawiki}
{{Collapse_bottom}}
```
-   こんな感じの結果が得られました．

![](1561457304275-Screenshot_from_2019-06-25_19-03-01.png "1561457304275-Screenshot_from_2019-06-25_19-03-01.png"){width="800"}

-   他方のカメラでEpipolar Line上に特徴点があることをわかります．
-   少しずれていますが，内部パラメータが雑だからです．
-   キャリブレーションをもっと頑張るとぴったり上を通ることでしょう．

### 姿勢復元

-   既に相対姿勢R,tからE行列が計算できることは示しました．
-   逆にE行列から(スケールの不定性があるが)R,tを復元することもできます．
-   F行列からE行列を求めることは内部パラメータKがわかればできますね．
-   というわけで導きます．

交代行列は等値非ゼロな特異値2つを持つ．

回転行列を掛けても特異値は変わらない．

すなわちE行列は交代行列と回転行列の積であるため，次のように特異値分解できる．

$E=U\Sigma V^T=U\left[\begin{matrix} s&0&0\\0&s&0\\0&0&0 \end{matrix}\right] V^T$

ここで行列Wを次のように定義する．

$W=\left[\begin{matrix} 0&-1&0\\1&0&0\\0&0&1 \end{matrix}\right]$

Wは直交行列行列になっている．

さらに$W\Sigma W^{-1}=\Sigma$が成立する．

すなわち$E=UW\Sigma U^T UW^T V^T$

ここで$UW\Sigma U^T$は交代行列を成し，$UW^TV^T$は交代行列を成す．

すなわち以下のように分解ができる．

$[\vec{t}]_{\times} = UW\Sigma U^T$

$R = UW^TV^T$

ここで$[\vec{t}]_{\times}\vec{t}=\vec{0}$を踏まえると，$U=\left[\vec{u_1}\vec{u_3}\vec{u_3}\right]$とかくとして，$\vec{t}\simeq\pm \vec{u_3}$となる．

-   これはtの正負とRの向きにより4パターンの不定性が残ります．

$R = UW^TV^T\ ;\  \vec{t}=+ \lambda \vec{u_3}$\
$R = UW^TV^T\ ;\  \vec{t}=- \lambda\vec{u_3}$\
$R = UWV^T\ ;\  \vec{t}=+ \lambda\vec{u_3}$\
$R = UWV^T\ ;\  \vec{t}=- \lambda\vec{u_3}$

-   $[\vec{t}]_{\times}$からスケールされた$\vec{t}$が得られそうですが，E行列を分解して正しいスケールを持った$\vec{t}$を得られることは普通ないので，計算が楽な$\vec{u_3}$をとる文化があります．
-   というのもE行列は全体を定数倍しても対象とするEpipolar幾何が変化しないという性質があります．
    -   $\vec{x_1}^T E \vec{x_2}=0$ですので．
-   そして，E行列を分解するシチュエーションというのは$[\vec{t}]_{\times}$のスケールが真値と異なることが多いので，わざわざ行列の掛け算をしてtを求めることはないということです．

```{=html}
<!-- -->
```
-   4パターンの不定性は投影方向を加味することで一意に定まります．以下の図をご覧ください．
    -   赤い点が対象物．ピンクと青の点がカメラ座標系の原点で，T字のイラストが画像平面だと思ってください．
    -   それぞれのT字の相対位置は，並進方向と回転方向の正負の組み合わせになっています．
    -   左から2番目意外は，カメラの後方から投影されているのが読み取れると思います．すなわち不適切です．

![](1561513431836-Screenshot_from_2019-06-26_10-43-03.png "1561513431836-Screenshot_from_2019-06-26_10-43-03.png"){width="800"}

-   OpenCVにはcv::RecoverPoseという関数がありますのでそちらをご利用ください．
    -   実際に動くサンプルは次項で扱います．
-   [APIの詳細はこちら](https://docs.opencv.org/3.4.3/d9/d0c/group__calib3d.html#gadb7d2dfcc184c1d2f496d8639f4371c0)
    -   この関数では先の4パターンの不定性は特徴点がどこに投影されるかを踏まえることで，うまく解消されています．
-   以下はOpenCV ver3.4の *modules/calib3d/src/five-point.cpp* の
    *cv::recoverPose(\...)* の中身です．

![](1561687762634-Screenshot_from_2019-06-28_11-07-15.png "1561687762634-Screenshot_from_2019-06-28_11-07-15.png"){width="400"}

### 8点法

-   もし相対姿勢の情報を使わずにE行列・F行列を生成できればどうなるでしょうか?
-   E行列は先の項で回転行列と並進ベクトルに分解できるとを示したので，そんなことができれば撮影画像からカメラ間の相対位置が推定できてしまいます．
-   できます．
-   具体的には，双方の画像で特徴点マッチングが十分な個数取ることで実現可能です．
-   以下ではカメラ1とカメラ2である程度同じシーンを見ている状況を想定します．
    -   ある程度同じシーンを見ているというのは，まったく違うところを見ている状況ではE行列など推定しようがないので，とりあえず課した制約です．
    -   visual odometryとかが目的なら普通は満たされています．

```{=html}
<!-- -->
```
-   (画像平面における)Epipolar Equationは以下のようにかけます．

$\left[x_{i1} y_{i1} 1\right] 
\left[\begin{matrix}
F_{11}& F_{12}&F_{13}\\
F_{21}& F_{22}&F_{23}\\
F_{31}& F_{32}&F_{33}\\
\end{matrix}\right]
\left[\begin{matrix}
x_{i2}&\\y_{i2}\\1
\end{matrix}\right]=0$

-   $x_{i1}^{(j)},y_{i1}^{(j)}$を1番目のカメラで検出できたj番目の特徴点の座標としましょう．
-   特徴点はn個とれたとします．
-   そしてそれに対応する2番目のカメラでの特徴点の座標を$x_{i2}^{(j)},y_{i2}^{(j)}$としましょう．
-   さすれば，特徴点のペアの数だけEpipolar
    Equationをたてることができます．
-   それらを以下のように書き並べてみましょう．

$\left[\begin{matrix}
x_{i1}^{(1)}x_{i2}^{(1)} & x_{i1}^{(1)}y_{i2}^{(1)} & x_{i1}^{(1)} & y_{i1}^{(1)}x_{i2}^{(1)} & y_{i1}^{(1)}y_{i2}^{(1)} & y_{i1}^{(1)}&x_{i2}^{(1)}&y_{i2}^{(1)}&1\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
x_{i1}^{(n)}x_{i2}^{(n)} & x_{i1}^{(n)}y_{i2}^{(n)} & x_{i1}^{(n)} & y_{i1}^{(n)}x_{i2}^{(n)} & y_{i1}^{(n)}y_{i2}^{(n)} & y_{i1}^{(n)}&x_{i2}^{(n)}&y_{i2}^{(n)}&1\\
\end{matrix}\right] 
\left[\begin{matrix}
F_{11}\\F_{12}\\F_{13}\\
F_{21}\\F_{22}\\F_{23}\\
F_{31}\\F_{32}\\F_{33}\\
\end{matrix}\right]=\vec{0}$

-   このままではF=0の自明な解がじゃまなので，E行列，F行列が定数倍されてもEpipolar方程式は変わらないという性質を利用します．

$\left[\begin{matrix}
x_{i1}^{(1)}x_{i2}^{(1)} & x_{i1}^{(1)}y_{i2}^{(1)} & x_{i1}^{(1)} & y_{i1}^{(1)}x_{i2}^{(1)} & y_{i1}^{(1)}y_{i2}^{(1)} & y_{i1}^{(1)}&x_{i2}^{(1)}&y_{i2}^{(1)}&1\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
x_{i1}^{(n)}x_{i2}^{(n)} & x_{i1}^{(n)}y_{i2}^{(n)} & x_{i1}^{(n)} & y_{i1}^{(n)}x_{i2}^{(n)} & y_{i1}^{(n)}y_{i2}^{(n)} & y_{i1}^{(n)}&x_{i2}^{(n)}&y_{i2}^{(n)}&1\\
\end{matrix}\right] 
\left[\begin{matrix}
F_{11}\\F_{12}\\F_{13}\\
F_{21}\\F_{22}\\F_{23}\\
F_{31}\\F_{32}\\F_{33}=1\\
\end{matrix}\right]=\vec{0}$

-   としてやることで，素敵にFを求めることができます．
-   未知数が8個なので，少なくとも8点の特徴点ペアがあると実現できます．
-   このF行列を求める手法を*8点法*\'と呼びます．
-   あとは予めキャリブレーションして求めておいた内部パラメータKを用いればE行列を計算できます．

```{=html}
<!-- -->
```
-   ところで，特徴点のマッチングからF行列を求める手法は沢山提案されており，たしか現在の最小は5個のマッチングでF行列を求められたような気がします．

```{=html}
<!-- -->
```
-   というわけで，カメラ間の相対位置情報を無しにF(E)行列を計算することに成功しました．

#### サンプル {#サンプル_2}

-   ステレオカメラで得られた以下の2枚の画像間でE行列を算出し，カメラの並進移動がX軸に平行であることを確認します．

![](1561887234471-left2.png "1561887234471-left2.png"){width="400"}![](1561887234480-right2.png "1561887234480-right2.png"){width="400"}

-   OpenCVのAPIからcv::findEssentialMatとcv::recoverPoseを利用します．
    -   cv::findEssentialMatにはマッチングの取った点の組と内部パラメータを渡します．
    -   cv::recoverPoseはE行列とその他もろもろを与えると回転行列Rと並進ベクトルtを返してくれます．
        -   ただしtはスケールが不定であることに注意してください．
-   内部パラメータは以下を使います．
    -   $K_1=K_2=\left( \begin{array}{ccc} f_x & 0 & c_x \\  0 & f_y & c_y\\ 0 & 0 & 1\end{array} \right)=\left( \begin{array}{ccc} 350 & 0 & 336 \\  0 & 350 & 188\\ 0 & 0 & 1\end{array} \right)$\[pixel\]

```{=mediawiki}
{{Collapse_top|サンプル}}
```
```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    const cv::Mat K = (cv::Mat1d(3, 3) << 350, 0, 336, 0, 350, 188, 0, 0, 1);  // ZEDの内部パラメータ
    cv::Mat image1 = cv::imread("left2.png"), image2 = cv::imread("right2.png");
    auto [points1, points2] = getMatching(image1, image2);

    cv::Mat R, t, mask; // maskはRANSACで発見したハズレ値をrecoverPoseで利用しないように参照される 
    cv::Mat E = cv::findEssentialMat(points1, points2, K, cv::RANSAC, 0.999, 1.0, mask);
    cv::recoverPose(E, points1, points2, K, R, t, mask);

    std::cout << "Essential Matrix:\n" << E << "\nRotation Matrix:\n"<< R << "\ntranslation vector:\n"<< t.t() << std::endl;
    return 0;
}
```

```{=mediawiki}
{{Collapse_bottom}}
```
-   上のサンプルを実行したところ以下のような結果が得られました．

`Essential Matrix:`\
`[0.0002379468328142012, -0.0168795298850736, -0.01029923916574708;`\
` 0.03138060972822553, -0.001506215308047754, -0.7063328560481184;`\
` 0.009268528464591743, 0.7068437793088128, -0.001342264036755218]`\
`Rotation Matrix:`\
`[0.9997876370431887, 0.001555573472214729, 0.02054899040755739;`\
` -0.001510690161840784, 0.9999964398852775, -0.00219955268300016;`\
` -0.02055233881660001, 0.002168042421844314, 0.999786427674043]`\
`translation vector:`\
`[-0.9996088740350083, 0.01462088288357625, -0.02383964626766797]`

-   E行列は見てもとくに何もわかりませんが，回転行列がほぼ単位行列(画像間でカメラ姿勢は回転していない)であることは読み取れると思います．
-   また並進ベクトルがだいたいx軸に並行であることがわかります．
    -   何度でも言いますが，具体的にどれだけX軸に沿って移動したかはこの画像情報だけではわかりません．

### まとめ

-   そろそろ自己位置推定ができそうな気配がでてきましたね．
-   今のままでは並進移動のスケールが未知なので困ってしまいます．
-   [このスライドの該当ページを見るのが速い](https://docs.google.com/presentation/d/1WGq4eIUgLeIXhyGiUKm_zmSK3LDjqNLDZc9SAlyIbu8/edit?usp=sharing)

### 練習 {#練習_15}

-   相対姿勢が未知のカメラ間で特徴点マッチングを利用してE行列を計算せよ．
-   そして回転角(ピッチ角)と並進移動量を調べよ．
-   画像は以下．

![](1561457265891-left1.png "1561457265891-left1.png"){width="400"}![](1561457265897-right1.png "1561457265897-right1.png"){width="400"}

-   カメラの内部パラメータは以下を用いよ．

$K=\left(\begin{matrix} 
500&0&320\\
0&500&240\\
0&0&1
\end{matrix}\right)$\
$D=\left(-0.15,0.83,-2.7e-4,-1.2e-3,-2.3\right)^T$\
![`1561457356425-DSC_1437.jpg`](1561457356425-DSC_1437.jpg "1561457356425-DSC_1437.jpg"){width="200"}

```{=mediawiki}
{{Collapse_top|サンプルコード}}
```
-   8点法のサンプルコードとほぼ同じになるので割愛．

```{=mediawiki}
{{Collapse_bottom}}
```
-   OpenCVは微妙に行列演算が扱いづらいのでEigenを併用すると人生が捗るかもしれません．
-   cv::MatとEigen::Matrixは以下のような相互変換が可能です．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <Eigen/Dense>
#include <iostream>
#include <opencv2/core/eigen.hpp>
int main()
{
    Eigen::Matrix3d R = Eigen::Matrix3d::Identity();
    cv::Mat Rcv;
    cv::eigen2cv(R, Rcv);
    std::cout << "\ncv::Mat\n" << Rcv << std::endl;
    cv::cv2eigen(Rmat, R);
    std::cout << "\nEigen::Matrix\n" << R << std::endl;
}
```

`ところで`[`近年のMMD動画`](https://www.youtube.com/watch?v=V-189K9CcQw&list=PLY4pDevDNKpNDH3Pp9fwYJ3YiKIUdb6jL&index=16)`はレベルが高いのでぜひ見てもらいたい．`

## Triangulation

-   [三角測量](https://ja.wikipedia.org/wiki/%E4%B8%89%E8%A7%92%E6%B8%AC%E9%87%8F)です．
    -   英語で書くとかっこいいですね．
-   3次元復元の界隈ではカメラは角度を測るセンサだと表現されることがあります．
-   特徴点をとるときくらいしか色情報は使わず，投影される座標ばかりを気にすることが多いからでしょう．
-   相対姿勢が既知のカメラが二個以上あるならば，角度だけでなく距離が測れます．

### 並行ステレオ

-   カメラが並行に並んでいるシンプルなケースです．
-   以下の図のようにカメラが配置されているのを想像してください．

![](1561893126201-Screenshot_from_2019-06-30_20-07-30.png "1561893126201-Screenshot_from_2019-06-30_20-07-30.png"){width="600"}

-   緑色のカメラはワールド座標の原点に+zの方向を向いています．
-   青色のカメラは$\vec{t}=(b,0,0)^T$で+zの方向を向いています．
-   上の方にある赤い点は3次元空間に存在する特徴点だとしましょう．
-   特徴点を$z_c=f$の画像平面に投影したものが，(内側２つの)緑と青の点です．
-   図の外側にある緑と青の点は他方のカメラで観測した画像平面座標を，他方のカメラに写したものです．
-   いくつか定義をします．

```{=html}
<!-- -->
```
-   カメラ間隔を**ベースライン(長)**と呼びます．ここではbと書きます．
-   画像平面上で，観測された特徴点の座標の間隔を**disparity**と呼び，日本語では**視差**と呼ばれます．ここではdと書きます．
    -   b-dとdの違いに注意してください．
    -   ところで「視差」の英訳には「disparity」と「parallax」がありますが，使い分けが存在するらしいです．[wikipedia](https://ja.wikipedia.org/wiki/%E4%B8%A1%E7%9C%BC%E8%A6%96%E5%B7%AE)
-   特徴点までの距離$z_c$を**depth**と呼び，日本語では**深度**と呼ばれます．

```{=html}
<!-- -->
```
-   図を見ればすぐに気づけると思いますが，三角形に関する相似な関係がありますよね．
-   相似関係から次が言えます．
-   $(z_c-f):z_c = (b-d) : b$
-   すなわち $z_c=\frac{f\cdot b}{d}$

`<font size=6>`{=html}$\mathrm{Depth} = \frac{\mathrm{FocalLength} \cdot \mathrm{Base line}}{\mathrm{Disparity}}$`</font>`{=html}

-   これはとても本質を述べている式です．

`「depthはdisparityに反比例する」．`

-   特徴点が無限遠にあるときに，視差が0になります．
-   ディジタル画像の場合，視差は離散値になりがちなので特徴点が遠くなるほど，測定精度は悪くなることが予想されます．
    -   特徴点の座標がサブピクセル精度で得られることもあるので，分解能は問題によってまちまちです．

`「遠くまで計測したければ`$f\cdot b$`が大きくないといけない」`

-   カメラ間隔が長くないと遠くまで測れません．
    -   ただし，長くなればなるほど，両カメラの視界の共通集合は小さくなります．
-   焦点距離を大きくすると遠くまで測れますが，画角が狭くなります．
-   カメラ間隔を大きくととれないなら，割とどうしようもないことがあるわけです．
    -   すなわち，要件を満たすカメラ存在しないとか．

```{=html}
<!-- -->
```
-   これらのことは，並行ステレオの場合だけでなく一般の場合にも成り立ちます．
-   ちなみに単位は$z_c[m]=\frac{f[pixel] \cdot b[m]}{d[pixel]}$のような関係になることでしょう．

```{=html}
<!-- -->
```
-   ここでは深度しか求めていませんが，特徴点の座標は容易に復元可能です．
-   例えば，原点に配置されたカメラで$(x_i,y_i)$に特徴点を観測したとしましょう．さすれば，

$x_c=(x_i-c_x)/f_x \cdot z_c\\
 y_c=(x_y-c_y)/f_y \cdot z_c$

-   となります．cやfは内部パラメータです．

### 一般の場合

-   次は一般の場合での三角測量を行いましょう．
-   ここでは単にカメラの角度が異なるだけでなく，マッチングの取れた画素を逆投影しても交わらない場合を考えます．

![](1561990093536-Screenshot_from_2019-07-01_23-07-51.png "1561990093536-Screenshot_from_2019-07-01_23-07-51.png"){width="700"}

-   おわかりいただけるだろうか，
-   今，赤い点を2台のカメラで観測しようとしています．(図では橙点と緑点の間にあるように見えますが，意味はあまりないです．)
    -   カメラ1ではそれは$(x_i,y_i)$に写っており，カメラ2では$(x'_i,y'_i)$に写っています．
-   しかし，それらを逆投影した光軸が交わりません．
-   このような状況で，$\vec{x_w}$が満たすべき式をあたえます．

$\tilde{\vec{x_i}}\simeq P \tilde{\vec{x_w}} 
 \Rightarrow
 \tilde{\vec{x_i}} \times P\tilde{\vec{x_w}}=\vec{0} 
 \Rightarrow
 \left[\begin{matrix} x_{i}\\y_{i}\\1 \end{matrix}\right] \times
 \left[\begin{matrix} \vec{P_1}^T\\\vec{P_2}^T \\ \vec{P_3}^T \end{matrix}\right]
 \tilde{\vec{x_{w}}} = \vec{0}
 \Rightarrow
 \left[\begin{matrix} y_i\vec{P_1}^T-\vec{P_2}^T\\ \vec{P_1}^T-x_i\vec{P_3}^T\\  x_i\vec{P_2}^T-y_i\vec{P_1}^T \end{matrix}\right]
 \tilde{\vec{x_{w}}} = \vec{0}
 \Rightarrow
 \left[\begin{matrix} y_i\vec{P_3}^T-\vec{P_2}^T\\ \vec{P_1}^T-x_i\vec{P_3}^T \end{matrix}\right]
 \tilde{\vec{x_{w}}} = \vec{0}$

-   面倒だったので，必要十分条件も⇨で書いてます．
-   $P= \left[\begin{matrix} \vec{P_1}^T\\\vec{P_2}^T \\ \vec{P_3}^T \end{matrix}\right]$としています．
-   最後の変形で1行消えていますが，次が成り立つからです．
    -   $x_i=\frac{\vec{P_1}^T\vec{x_w}}{\vec{P_3}^T\vec{x_w}}\ \  y_i=\frac{\vec{P_2}^T\vec{x_w}}{\vec{P_3}^T\vec{x_w}}$

```{=html}
<!-- -->
```
-   というわけで，Pを用いて方程式が2本建てられたので，P\'を用いて同様のことをやると次が得られます．

$\left[\begin{matrix} y_i\vec{P_3}^T-\vec{P_2}^T\\ x_i\vec{P_3}^T-\vec{P_1}^T\\ y'_i\vec{P_3^{\prime}}^T-\vec{P_2^{\prime }}^T\\ x'_i\vec{P_3^{\prime}}^T-\vec{P_1^{\prime}}^T\end{matrix}\right]
  \left[\begin{matrix}x_w\\y_w\\z_w\\1\end{matrix}\right] = \vec{0}$

-   未知数が3に対して方程式が4本あるので，これを用いれば解が求められそうです．

```{=html}
<!-- -->
```
-   一般の場合に$P=K[I|\vec{0}],\ P'=K'[I|(b,0,0)^T]$を代入すると並行ステレオのあの式がでてきます．

```{=mediawiki}
{{Collapse_top|一般から並行ステレオへ}}
```
`まず前提は`\
$P=K[I|\vec{0}]=[K|\vec{0}]\\
 P'=K'[I|(b,0,0)^T]=[K'|(f'_x\cdot b,0,0)^T]$\
`というわけで，`\
$\vec{P_1}^T =(f_x,0,c_x,0)\ 
 \vec{P_2}^T =(0,f_y,c_y,0)\ 
 \vec{P_3}^T =(0,0,1,0)\\
 \vec{P'_1}^T=(f'_x,0,c'_x,f'_x\cdot b)\ 
 \vec{P'_2}^T=(0,f'_y,c'_y,0)\ 
 \vec{P'_3}^T=(0,0,1,0)$\
`改めまして一般の式は`\
$\left[\begin{matrix}
 y_i\vec{P_3}^T-\vec{P_2}^T\\
 x_i\vec{P_3}^T-\vec{P_1}^T\\
 y'_i\vec{P_3^{\prime}}^T-\vec{P_2^{\prime }}^T\\
 x'_i\vec{P_3^{\prime}}^T-\vec{P_1^{\prime}}^T
 \end{matrix}\right]
 \tilde{\vec{x_{w}}} =
 \vec{0}$\
`展開する`\
$\left[\begin{matrix}
 y_ip_{31}-p_{21} & y_ip_{32}-p_{22} & y_ip_{33}-p_{23} & y_ip_{34}-p_{24}\\
 x_ip_{31}-p_{11} & x_ip_{32}-p_{12} & x_ip_{33}-p_{13} & x_ip_{34}-p_{14}\\
 y'_ip'_{31}-p'_{21} & y'_ip'_{32}-p'_{22} & y'_ip'_{33}-p'_{23} & y'_ip'_{34}-p'_{24}\\
 x'_ip'_{31}-p'_{11} & x'_ip'_{32}-p_{12} & x'_ip'_{33}-p'_{13} & x'_ip'_{34}-p_{14} 
 \end{matrix}\right]
 \tilde{\vec{X_{w}}} =
 \vec{0}$\
`Pの構成を考えると`\
$\left[\begin{matrix}
 0 & -f_y & y_i-c_y & 0\\
 -f_x & 0 & x_i-c_x & 0\\
 0 & -f_y' & y'_i-c_y' & 0\\
 -f_x' & 0 & x'_i-c_x' & -b\cdot f_x' 
 \end{matrix}\right]
 \left[\begin{matrix}
 x_w\\
 y_w\\
 z_w\\
 1\end{matrix}\right] = \vec{0}$\
`綺麗にする`\
$y_i=f_x\frac{y_w}{z_w}+c_y\\
 x_i=f_x\frac{x_w}{z_w}+c_x\\
 y'_i=f'_x\frac{y_w}{z_w}+c'_y\\
 x'_i=f'_x\frac{x_w+b}{z_w}+c'_x\\$\
`すなわち`\
$z_w=\frac{b\cdot f}{x_i-x_i'}$

```{=mediawiki}
{{Collapse_bottom}}
```
### Pure Rotation {#pure_rotation}

-   すなわち t=(0,0,0)\^T
-   初めて聞いたときに「なーにが『ピュア』やねん．プリキュアかよ．」と思いましたが，そういう表現する文化があります．
    -   例えば[こちら](https://scholar.google.co.jp/scholar?hl=ja&as_sdt=0%2C5&as_vis=1&q=visual+SLAM+pure+rotation&btnG=)．
-   E行列の定義を見てもらえればわかりますが，pure
    rotation時はE行列は意味がなくなります．
    -   E行列が死ぬのならば，姿勢復元もできません．
-   また移動していない，ベースライン長が０なので，三角測量ができません．
-   何が言いたいかというと，visual-SLAMはその幾何的性質から，pure
    rotationに弱いです．

### 練習 {#練習_16}

-   適当に局所特徴量を取り出して，その点の3次元座標を求めよ．
    -   OpenCVには**cv::triangulation**という関数がある．
        -   が引数と返値がキモいので注意して運用すると良い．
        -   並行ステレオの場合は例の式を使って自分で計算したほうがスマートになるのかもしれない．

![](1562163980702-left2.png "1562163980702-left2.png"){width="400"}![](1562163983170-right2.png "1562163983170-right2.png"){width="400"}

-   カメラのパラメータは次のとおりである．

`解像度(672,376)`\
$K_1=K_2=\left( \begin{array}{ccc} f_x & 0 & c_x \\  0 & f_y & c_y\\ 0 & 0 & 1\end{array} \right)=\left( \begin{array}{ccc} 350 & 0 & 336 \\  0 & 350 & 188\\ 0 & 0 & 1\end{array} \right)$`[pixel]`\
$\left[R_{12} |\vec{t_{12}}\right]=\left( \begin{array}{ccc} 1 & 0 & 0 & 0.12 \\  0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\end{array} \right)$`[m]`

```{=mediawiki}
{{Collapse_top|サンプル}}
```
```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
const cv::Mat K = (cv::Mat1f(3, 3) << 350, 0, 336, 0, 350, 188, 0, 0, 1);
const cv::Mat Rt1 = (cv::Mat1f(3, 4) << 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0);
const cv::Mat Rt2 = (cv::Mat1f(3, 4) << 1, 0, 0, 0.12, 0, 1, 0, 0, 0, 0, 1, 0);
const cv::Mat P1 = K * Rt1;
const cv::Mat P2 = K * Rt2;

int main()
{
    cv::Mat image2 = cv::imread("left2.png"), image1 = cv::imread("right2.png");
    auto [points1, points2] = getMatching(image1, image2); // はるか昔に定義したやつ
    cv::Mat homo3d, tmp;
    // 4xNの斉次ベクトルの列を3xNの非斉次ベクトルの列に変えたい すなわち(x,y,z,w) -> (x/w,y/w,z/w) のようにしたい
    cv::triangulatePoints(P1, P2, points1, points2, homo3d);  // 4xN(1ch)
    cv::convertPointsFromHomogeneous(homo3d.t(), tmp);        // Nx4(1ch) -> Nx1(3ch)
    tmp = tmp.reshape(1);                                     // Nx3(4ch) -> Nx3(1ch)
    tmp = tmp.t();                                            // Nx3(1ch) -> 3xN(1ch)
    cv::namedWindow("window", 0);
    for (int i = 0; i < tmp.cols; i += 10) {
        cv::Point3i p = 1000 * cv::Point3f(tmp.col(i));
        cv::Point2f i1 = points1.at(i);
        cv::Point2f i2 = points2.at(i);

        std::stringstream ss;
        ss << p;
        cv::Scalar color = cv::Scalar(rand() % 255, rand() % 255, rand() % 255);
        cv::circle(image1, i1, 3, color, -1);
        cv::addText(image1, ss.str(), i1, "Consolas", 10, color);
        cv::circle(image2, i2, 3, color, -1);
    }
    cv::Mat image;
    cv::hconcat(image1, image2, image);
    cv::imshow("window", image);
    cv::waitKey(0);
}
```

```{=mediawiki}
{{Collapse_bottom}}
```
-   サンプルの結果

![](1562163977923-hoge.png "1562163977923-hoge.png"){width="1200"}

-   おすすめの[Lemon](https://www.youtube.com/watch?v=_0na0_Ysdvs&list=PLY4pDevDNKpNDH3Pp9fwYJ3YiKIUdb6jL&index=19&t=0s)です

## Perspective n Points {#perspective_n_points}

-   特徴点の三次元座標が既知で，さらにそれらが画像上のどこに投影されたかがわかれば，カメラの姿勢(位置＋角度)が計算できるというものです．
-   例えば，ARマーカのようなランドマークがあるような場合や，Triangulationによって測距ずみの特徴点などがある場合に適用できます．
-   日本語ではPnP問題とか呼ばれます．
    -   pnp型トランジスタが検索の邪魔になります．

![](1561893048839-Screenshot_from_2019-06-30_20-10-24.png "1561893048839-Screenshot_from_2019-06-30_20-10-24.png"){width="600"}

-   上の図は3次元空間に存在する3点$\vec{x_{w1}},\vec{x_{w2}},\vec{x_{w3}}$をカメラで撮影したところ，画素$\vec{x_{i1}},\vec{x_{i2}},\vec{x_{i3}}$に投影された様子を示しています．
-   それらが既知の条件下で，位置$R,\vec{t}$を求めたいです．

### カメラパラメータPの導出

-   まずカメラ行列$P=K[R|\vec{t}]$を求めることを目標にします．

カメラ行列の定義から，
$\left[\begin{matrix} \vec{x_i}\\1 \end{matrix}\right] \simeq P \left[\begin{matrix} \vec{x_w}\\1 \end{matrix}\right]$

となる．ここにスケールファクタ$\lambda$を導入して等号にする.

$P \left[\begin{matrix} \vec{x_w}\\1 \end{matrix}\right]-\lambda \left[\begin{matrix} \vec{x_i}\\1 \end{matrix}\right]=\vec{0}\\$

3x4の行列Pの中身を$\left[\begin{matrix}\vec{P_1}^T\\\vec{P_2}^T\\\vec{P_3}^T\\\end{matrix}\right]\$と定義して，

$\vec{P_i}^T$(1x4ベクトル)の転地が$\vec{P_i}$(4x1ベクトル)であることに注意して，素敵に式変形する．

$\left[\begin{matrix}
\vec{x_w}^T & 1 & \vec{0}^T & 0 & \vec{0}^T &  0 & -x_i\\
\vec{0}^T      &  0 & \vec{x_w}^T& 1& \vec{0}^T & 0 & -y_i\\
\vec{0}^T      &  0  & \vec{0}^T &  0   & \vec{x_w} & 1 & -1\\
\end{matrix}\right]\
\left[\begin{matrix}
\vec{P_1}\\
\vec{P_2}\\
\vec{P_3}\\
\lambda
\end{matrix}\right]=\vec{0}$

上の式に登場する$\vec{0}^T$は全て3次の列ベクトルであることに注意されたい．

-   ここまでで，未知のカメラパラメータ12個+スケールファクタ1個に対して，3本の方程式を建てられました．
-   あとは，これを複数の組に適応することで有決定な問題をつくることができます．
-   例えば，n個の組が得られたとしましょう(上の図ではn=3)
-   あとはるか昔に定義した次の表現を利用します．$\tilde{\vec{x_w}}^T=(x,y,z,1)$

$\left[\begin{matrix}
\tilde{\vec{x_{w1}}}^T&\vec{0}^T&\vec{0}^T&-x_{i1}&\dots&0\\
\vec{0}^T&\tilde{\vec{x_{w1}}}^T&\vec{0}^T&-y_{i1}&\dots&0\\
\vec{0}^T&\vec{0}^T&\tilde{\vec{x_{w1}}}^T&-1&\dots&0\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\\
\tilde{\vec{x_{wn}}}^T&\vec{0}^T&\vec{0}^T&0&\dots&-x_{in}\\
\vec{0}^T&\tilde{\vec{x_{wn}}}^T&\vec{0}^T&0&\dots&-y_{in}\\
\vec{0}^T&\vec{0}^T&\tilde{\vec{x_{wn}}}^T&0&\dots&-1\\

\end{matrix}\right]\
\left[\begin{matrix}
\vec{P_1}\\
\vec{P_2}\\
\vec{P_3}\\
\lambda_1\\
\vdots\\
\lambda_n\\
\end{matrix}\right]=\vec{0}$

-   n個のペアが得られたときの未知数の数は(12+n)個，そのときの方程式の数は3n個なので，
-   解が定まる条件は，$(12+n)\leq 3n \Rightarrow 6\leq n$となります．
-   そういう経緯もあって，「P6P problem」と呼ばれることもあるようです．

`僕は今`[`この動画`](https://www.youtube.com/watch?v=rJj0JmdxrAo)`を見ています．`

### PからK,R,tの導出

-   次はPを内部パラメータKと外部パラメータ$R|\vec{t}$に分解することを目指しましょう．

$P = K[R|\vec{t}] = [KR|K\vec{t}]$

と書け，前段は3x3の行列，後段は3x1のベクトルになる．

前半に注目すると，

$KR= \left[\begin{matrix}
f_x&0&c_x\\
0&f_y&c_y\\
0&0&1\\
\end{matrix}\right]
\left[\begin{matrix}
R_{11}&R_{12}&R_{13}\\
R_{21}&R_{22}&R_{23}\\
R_{31}&R_{32}&R_{33}\\
\end{matrix}\right]$

上三角行列x直交行列という見方ができる．

ところで，行列を上三角行列と直交行列の積に分解することはRQ分解と呼ばれ，
任意の行列に対して，RQ分解は一意であることが保証されている．

というわけでPの左側3x3領域に対しRQ分解を施し，得られた上三角行列をK，直交行列をRとし，ほじくり返すことで$\vec{t}$を得られる．

-   ここではQR分解(およびRQ分解)は深く言及しません．
-   なにはともあれ，カメラパラメータPが与えられれば各パラメータに分解できる術を得ることができました．

### サンプル {#サンプル_3}

-   以下の画像でチェスボードがカメラ座標系でどの位置にあるかを計算します．

![](1562484313892-chess_left0.png "1562484313892-chess_left0.png"){width="400"}

-   次の図のようなワールド座標系を設定します．

![](1562484317988-Screenshot_from_2019-07-07_16-25-01.png "1562484317988-Screenshot_from_2019-07-07_16-25-01.png"){width="200"}

-   するとチェス盤の角は *cv::Point3f(i\* SIZE, -j\* SIZE,
    0));*な感じで定まります．
    -   SIZEはチェス盤の一マスの一辺の長さです．
    -   iとjの向きはいい感じに決めます．
-   ここでは画像中のチェス盤の角を検出する方法として，*cv::findChessboardCorners*を利用します．
-   それで得られた角の座標と，ワールド座標におけるチェス盤の座標を用いてPnP問題をときます．
-   PnP問題を解くときはOpenCVでは*cv::solvePnP*が利用できます．
    -   3次元点と2次元点の組み合わせにハズレ値が含まれている場合は*cv::solvePnPRansac*を用いると良いかもしれません．
    -   ここではfindChessboardCornersが頑強なので使いません．
-   *solvePnP*は姿勢と3x1のベクトル2本で出力します．
    -   1本は並進ベクトルで，もう一本が3次元での回転を表しています．
    -   この回転の方をどう呼ぶのが一般的なのか知らないんですが，(Lie代数$\mathfrak{so}(3)$とか，回転ベクトルとか呼ばれている)
    -   親しみやすい(?)回転行列に変換するために*cv::Rodriguis*を使うことができます．
        -   回転ベクトルと回転行列の相互変換ができます．

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
#include <opencv2/opencv.hpp>
int main()
{
    // カメラの内部パラメータ+歪みパラメータ
    const cv ::Mat K = (cv::Mat1f(3, 3) << 500, 0, 320, 0, 500, 240, 0, 0, 1);
    const cv ::Mat D = (cv::Mat1f(5, 1) << -0.15, 0.83, 0, 0, -2.3);
    float SIZE = 0.0195f;  // チェス盤の一マス長[m](19.5mm)

    cv::Mat img = cv::imread("chess_left0.png");
    std::vector<cv::Point3f> objects;
    for (int i = 0; i < 10; i++)
        for (int j = 0; j < 7; j++)
            objects.push_back(cv::Point3f(static_cast<float>(i) * SIZE, -static_cast<float>(j) * SIZE, 0));

    cv::Mat corners;  // CV_32FC2の[70x1]行列
    cv::findChessboardCorners(img, cv::Size(7, 10), corners, cv::CALIB_CB_ADAPTIVE_THRESH | cv::CALIB_CB_FILTER_QUADS | cv::CALIB_CB_NORMALIZE_IMAGE | cv::CALIB_CB_FAST_CHECK);
    cv::drawChessboardCorners(img, cv::Size(7, 10), corners, true);

    cv::Mat rvec, tvec;  // CV_32FC1の[3x1]行列
    cv::Mat R;             // CV_32FC1の[3x3]行列
    cv::solvePnP(objects, corners, K, D, rvec, tvec);
    cv::Rodrigues(rvec, R);  // 回転ベクトルを回転行列に変換
    std::cout << R << "\n\n" << tvec.t() << std::endl;

    cv::imshow("show", img);
    cv::waitKey(0);
}
```

-   出力は次のようになりました．

`[0.99350464, 0.053036947, -0.10067575;`\
`-0.045826644, 0.99630564, 0.072629422;`\
`0.10415585, -0.067544036, 0.99226475]`\
`[-0.048660289, 0.15813899, 0.75532591]`

-   あんまりまともに真値を測っていなかったのですが，カメラから見て前方700\[mm\]ぐらいだったんだと思います．
    -   チェス盤の左下は画面の中央より左，すなわちXは負．(正しい．)
    -   チェス盤の左下は画面の中央より下，すなわちYは正．(正しい．)
    -   チェス盤画面前方に見えている，すなわちZは正．(正しい．)
-   座標軸のとり方的に回転はほぼないと思いますので，出力された回転行列はおよそ妥当だと思います．

![](1562484908442-Screenshot_from_2019-07-07_16-34-28.png "1562484908442-Screenshot_from_2019-07-07_16-34-28.png"){width="200"}

-   これはわかりにくいのですが，左下の点から順に上方向に検出したことを意味しています．
    -   当然ながらこれと，上のプログラム中の*objects*の値の順番に齟齬があれば正しい姿勢は得られません．

### 練習 {#練習_17}

-   以下の写真は位置の異なる2台のカメラで撮影した画像であり，チェスボードは同じ場所にい続けている．カメラ間の相対姿勢をもとめよ．
-   ![](1562484596047-chess_left1.png "1562484596047-chess_left1.png"){width="400"}![](1562484596056-chess_right1.png "1562484596056-chess_right1.png"){width="400"}

```{=html}
<!-- -->
```
-   カメラの内部パラメータは以下を用いよ．

$K=\left(\begin{matrix} 
500&0&320\\
0&500&240\\
0&0&1
\end{matrix}\right)$\
$D=\left(-0.15,0.83,-2.7e-4,-1.2e-3,-2.3\right)^T$

-   回転はなく，X軸方向に120mmぐらい．

```{=mediawiki}
{{Collapse_top|サンプルコード}}
```
[livetune feat. 初音ミク「Tell Your
World」起用のCMがカンヌ国際広告祭で銅賞受賞
2012](https://www.barks.jp/news/?id=1000080757)

```{=html}
<div align="center">
```
main.cpp

```{=html}
</div>
```
``` {.cpp .numberLines}
```

```{=mediawiki}
{{Collapse_bottom}}
```
## Visual Odometry {#visual_odometry}

### 概要 {#概要_2}

-   feature basedとdirect methodがあります．
-   direct
    methodは実装とリアルタイム性の確保が難しいのでここでは紹介しません．
-   feature based visual odometryの流れを紹介し，実装していきます．
-   ここで実装するものは実用できるものはできません．あくまでたたき台・チュートリアルの成果物だと思ってください．
-   とりあえず，作ったものはこちら

### 流れ

-   カメラのキャリブレーション
-   特徴点抽出
-   1フレーム前とマッチング
-   E行列の推定
-   R,tに分解
-   測距
-   スケール計算

### 練習 {#練習_18}

![](1561310923798-D2gsTQkUkAAPOZk.jpg "1561310923798-D2gsTQkUkAAPOZk.jpg"){width="200"}

## Bundle Adjustment {#bundle_adjustment}

-   **初音ミク**（はつね ミク、Hatsune
    Miku）は、[クリプトン・フューチャー・メディア](クリプトン・フューチャー・メディア "wikilink")から発売されている[音声合成](音声合成 "wikilink")・[デスクトップミュージック](デスクトップミュージック "wikilink")
    (DTM)
    用のボーカル音源、およびその[キャラクター](キャラクター "wikilink")である

### 再投影誤差

-   観測した点を空間に逆投影するじゃん．もう一回投影するじゃん．それは元に戻るはずじゃん?

### Graph-SLAM {#graph_slam}

-   確率ロボティクスを見ろ．

### general graph optimization(g2o) {#general_graph_optimizationg2o}

-   楽しい．

### ceres solver {#ceres_solver}

-   やっぱやーめた．

### 練習 {#練習_19}

![](1561310923798-D2gsTQkUkAAPOZk.jpg "1561310923798-D2gsTQkUkAAPOZk.jpg"){width="200"}

## ICP

-   Iteretive Closest Points
-   点群でLocalizationしたい．
-   一番近い点どうしをペアづけして，それらの距離が小さくなるような操作を繰り返したら，そのうち収束するやろ
-   ここが整理されるのは8月に入ってからでしょうね．

### 練習 {#練習_20}

-   ここが整理されるのは8月に入ってからでしょうね．

## 参考文献

  ----------------------------------------------------------------- ---------------------------- ---------------------------
  書籍名                                                            出版社                       備考
  詳解OpenCV コンピュータビジョンライブラリを使った画像処理・認識   O\'REILLY JAPAN              分厚い
  コンピュータビジョン 広がる要素技術と応用                         共立出版                     新しい
  3次元ビジョン                                                     共立出版                     表紙がださい
  ディジタル画像処理                                                CG-ARTS協会                  図解が素敵
  Epipolar Geometry in Stereo, Motion and Object Recognition        Kluwer Academic Publishers   エピポーラ幾何に詳しい
  Multiple View Geometry in Computer Vision                         Cambridge University Press   強い
  三次元画像計測の基礎 バンドル調整の理論と実践                     東京電機大学出版局           Bundle Adjustmentに詳しい
                                                                                                 
  ----------------------------------------------------------------- ---------------------------- ---------------------------

pandoc version 3.1.2
